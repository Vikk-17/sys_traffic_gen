{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcUXuzQMLcqga6zXZZH3zW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikk-17/sys_traffic_gen/blob/main/integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfH3OyzaXP-B",
        "outputId": "985a6d0b-f37f-4f07-adb2-083813a2f1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 11/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 12/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 13/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 14/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 15/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 16/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 17/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 18/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 19/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: nan - val_loss: nan\n",
            "Epoch 20/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 21/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 22/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 23/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 24/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: nan - val_loss: nan\n",
            "Epoch 25/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: nan - val_loss: nan\n",
            "Epoch 26/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: nan - val_loss: nan\n",
            "Epoch 27/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: nan - val_loss: nan\n",
            "Epoch 28/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 29/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 30/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 31/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 32/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 33/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 34/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 35/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 36/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 37/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 38/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: nan - val_loss: nan\n",
            "Epoch 39/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 40/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: nan - val_loss: nan\n",
            "Epoch 41/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 42/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 43/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: nan - val_loss: nan\n",
            "Epoch 44/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: nan - val_loss: nan\n",
            "Epoch 45/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 46/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: nan - val_loss: nan\n",
            "Epoch 47/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: nan - val_loss: nan\n",
            "Epoch 48/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: nan - val_loss: nan\n",
            "Epoch 49/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 50/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "Generator Loss: 0.7108\n",
            "Discriminator Loss: -6.3539\n",
            "Epoch 20/100\n",
            "Generator Loss: 5.8910\n",
            "Discriminator Loss: 141.0199\n",
            "Epoch 30/100\n",
            "Generator Loss: 8.8161\n",
            "Discriminator Loss: -10.4322\n",
            "Epoch 40/100\n",
            "Generator Loss: 17.3662\n",
            "Discriminator Loss: -26.4788\n",
            "Epoch 50/100\n",
            "Generator Loss: 18.2339\n",
            "Discriminator Loss: -27.6218\n",
            "Epoch 60/100\n",
            "Generator Loss: 19.6676\n",
            "Discriminator Loss: -30.6323\n",
            "Epoch 70/100\n",
            "Generator Loss: 19.2788\n",
            "Discriminator Loss: -30.5002\n",
            "Epoch 80/100\n",
            "Generator Loss: 19.8164\n",
            "Discriminator Loss: -33.1102\n",
            "Epoch 90/100\n",
            "Generator Loss: 21.0842\n",
            "Discriminator Loss: -22.8223\n",
            "Epoch 100/100\n",
            "Generator Loss: 17.7450\n",
            "Discriminator Loss: -32.7886\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.4947 - loss: 41.7988 - val_accuracy: 0.4947 - val_loss: 6.7581\n",
            "Epoch 2/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5321 - loss: 6.1530 - val_accuracy: 0.6067 - val_loss: 9.3835\n",
            "Epoch 3/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5118 - loss: 5.1341 - val_accuracy: 0.6053 - val_loss: 8.9304\n",
            "Epoch 4/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5516 - loss: 5.6211 - val_accuracy: 0.5373 - val_loss: 3.4153\n",
            "Epoch 5/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5413 - loss: 4.1588 - val_accuracy: 0.6160 - val_loss: 3.5691\n",
            "Epoch 6/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5470 - loss: 2.6755 - val_accuracy: 0.6093 - val_loss: 1.8437\n",
            "Epoch 7/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5552 - loss: 2.3027 - val_accuracy: 0.4093 - val_loss: 2.5010\n",
            "Epoch 8/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5494 - loss: 2.0633 - val_accuracy: 0.4347 - val_loss: 4.1978\n",
            "Epoch 9/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5460 - loss: 2.6235 - val_accuracy: 0.4573 - val_loss: 1.7788\n",
            "Epoch 10/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5410 - loss: 2.2072 - val_accuracy: 0.6267 - val_loss: 1.7324\n",
            "Epoch 11/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5543 - loss: 1.4058 - val_accuracy: 0.6173 - val_loss: 1.6231\n",
            "Epoch 12/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5473 - loss: 1.5842 - val_accuracy: 0.5147 - val_loss: 1.7785\n",
            "Epoch 13/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5553 - loss: 1.7096 - val_accuracy: 0.6107 - val_loss: 1.6542\n",
            "Epoch 14/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5666 - loss: 1.3611 - val_accuracy: 0.4120 - val_loss: 2.3405\n",
            "Epoch 15/50\n",
            "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5471 - loss: 1.4307 - val_accuracy: 0.5013 - val_loss: 1.1792\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6079 - loss: 1.8878\n",
            "Test accuracy: 0.5987\n",
            "Warning: Skipping inverse transform for column 'src_ip' due to ValueError.\n",
            "Warning: Skipping inverse transform for column 'dst_ip' due to ValueError.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "# Global variables\n",
        "scaler = None\n",
        "label_encoders = {}\n",
        "\n",
        "def preprocess_csv_with_dummies(file_path, is_training=True):\n",
        "    global scaler, label_encoders\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Detect numerical and categorical columns\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Handle categorical columns with LabelEncoder instead of one-hot encoding\n",
        "    for col in categorical_columns:\n",
        "        if is_training:\n",
        "            label_encoders[col] = LabelEncoder()\n",
        "            df[col] = label_encoders[col].fit_transform(df[col])\n",
        "        else:\n",
        "            df[col] = label_encoders[col].transform(df[col])\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    if numerical_columns:\n",
        "        if is_training:\n",
        "            scaler = MinMaxScaler()\n",
        "            df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "        else:\n",
        "            df[numerical_columns] = scaler.transform(df[numerical_columns])\n",
        "\n",
        "    return df.astype(np.float32), numerical_columns, categorical_columns\n",
        "\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, original_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = self.build_encoder(original_dim, latent_dim)\n",
        "        self.decoder = self.build_decoder(original_dim, latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def build_encoder(self, original_dim, latent_dim):\n",
        "        inputs = tf.keras.layers.Input(shape=(original_dim,))\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return tf.keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def build_decoder(self, original_dim, latent_dim):\n",
        "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(latent_inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        outputs = tf.keras.layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "        return tf.keras.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = tf.keras.losses.mse(inputs, reconstructed)\n",
        "        reconstruction_loss *= tf.cast(tf.shape(inputs)[1], tf.float32)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(data_dim, activation='tanh')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Classifier(tf.keras.Model):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "def gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    alpha = tf.random.uniform([tf.shape(real_samples)[0], 1], 0.0, 1.0)\n",
        "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated)\n",
        "        predictions = discriminator(interpolated)\n",
        "\n",
        "    gradients = tape.gradient(predictions, interpolated)\n",
        "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
        "    gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))\n",
        "    return gradient_penalty\n",
        "\n",
        "def train_wgan(generator, discriminator, data, latent_dim, batch_size=64, epochs=100, n_critic=5, gp_weight=10.0):\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(n_critic):\n",
        "            # Train Discriminator\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                batch_indices = np.random.randint(0, data.shape[0], batch_size)\n",
        "                real_data = data[batch_indices]\n",
        "\n",
        "                z = tf.random.normal((batch_size, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "\n",
        "                real_output = discriminator(real_data, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                gp = gradient_penalty(discriminator, real_data, fake_data)\n",
        "                disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + gp_weight * gp\n",
        "\n",
        "            grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
        "\n",
        "        # Train Generator\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            z = tf.random.normal((batch_size, latent_dim))\n",
        "            fake_data = generator(z, training=True)\n",
        "            fake_output = discriminator(fake_data, training=True)\n",
        "            gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "        grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "            print(f\"Generator Loss: {gen_loss:.4f}\")\n",
        "            print(f\"Discriminator Loss: {disc_loss:.4f}\")\n",
        "\n",
        "def train_classifier(classifier, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
        "    classifier.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = classifier.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def predict_traffic(classifier, data, numerical_columns, categorical_columns):\n",
        "    \"\"\"\n",
        "    Predict traffic classification for new data\n",
        "    \"\"\"\n",
        "    processed_data, _, _ = preprocess_csv_with_dummies(data, is_training=False)\n",
        "    predictions = classifier.predict(processed_data)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Convert numeric predictions back to original labels\n",
        "    original_labels = label_encoders['classification'].inverse_transform(predicted_classes)\n",
        "\n",
        "    return original_labels\n",
        "\n",
        "\n",
        "\n",
        "def denormalize_data(synthetic_data, original_df, numerical_columns, categorical_columns, label_encoders, scaler):\n",
        "    \"\"\"\n",
        "    Denormalize synthetic data to match original data format and ranges\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(synthetic_data, columns=original_df.columns)\n",
        "\n",
        "    # Denormalize numerical columns\n",
        "    if numerical_columns and scaler:\n",
        "        df[numerical_columns] = scaler.inverse_transform(df[numerical_columns])\n",
        "\n",
        "        # Ensure priority remains in the range [1,5]\n",
        "        if 'priority' in df.columns:\n",
        "            df['priority'] = np.round(df['priority']).astype(int).clip(1, 5)\n",
        "\n",
        "        # Ensure port numbers are valid\n",
        "        for col in ['src_port', 'dst_port']:\n",
        "            if col in df.columns:\n",
        "                df[col] = np.round(df[col]).astype(int).clip(0, 65535)\n",
        "\n",
        "    # Convert categorical columns back to original categories\n",
        "    # Exclude timestamp from this process\n",
        "    categorical_columns_excluding_timestamp = [col for col in categorical_columns if col != 'timestamp']\n",
        "    for col in categorical_columns_excluding_timestamp:\n",
        "        if col in label_encoders:\n",
        "            try:\n",
        "                df[col] = label_encoders[col].inverse_transform(df[col].astype(int))\n",
        "            except ValueError:\n",
        "                # If casting to int fails, handle the column differently (e.g., skip or use a custom transformation)\n",
        "                print(f\"Warning: Skipping inverse transform for column '{col}' due to ValueError.\")\n",
        "\n",
        "    # Ensure timestamp format is maintained (if 'timestamp' in df.columns)\n",
        "    if 'timestamp' in df.columns:\n",
        "        #df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%m/%d-%H:%M:%S.%f') #Commented out for now because it may be encoded.\n",
        "\n",
        "        #If the timestamp is already in a string format, just ensure the format remains the same\n",
        "        if df['timestamp'].dtype == object:\n",
        "            pass\n",
        "        else: #If the timestamp is encoded, then we decode it.\n",
        "            df['timestamp'] = label_encoders['timestamp'].inverse_transform(df['timestamp'].astype(int))\n",
        "\n",
        "    # Validate protocol values\n",
        "    if 'protocol' in df.columns:\n",
        "        valid_protocols = ['TCP', 'UDP', 'ICMP']\n",
        "        df['protocol'] = df['protocol'].apply(lambda x: x if x in valid_protocols else np.random.choice(valid_protocols))\n",
        "\n",
        "    # Preserve src_ip and dst_ip as is\n",
        "    if 'src_ip' in df.columns and 'dst_ip' in df.columns:\n",
        "        df['src_ip'] = original_df['src_ip'].values[:len(df)]\n",
        "        df['dst_ip'] = original_df['dst_ip'].values[:len(df)]\n",
        "\n",
        "    # Ensure classification and alert retain valid values\n",
        "    valid_classifications = original_df['classification'].unique().tolist()\n",
        "    if 'classification' in df.columns:\n",
        "        df['classification'] = df['classification'].apply(lambda x: x if x in valid_classifications else np.random.choice(valid_classifications))\n",
        "\n",
        "    valid_alerts = original_df['alert'].unique().tolist()\n",
        "    if 'alert' in df.columns:\n",
        "        df['alert'] = df['alert'].apply(lambda x: x if x in valid_alerts else np.random.choice(valid_alerts))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_synthetic_data(num_samples, original_df):\n",
        "    \"\"\"\n",
        "    Generate synthetic network traffic data similar to the original dataset\n",
        "    \"\"\"\n",
        "    synthetic_data = pd.DataFrame()\n",
        "\n",
        "    # Generate timestamps\n",
        "    start_time = datetime.datetime.strptime(\"02/14-00:00:00.000000\", \"%m/%d-%H:%M:%S.%f\")\n",
        "    synthetic_data['timestamp'] = [(start_time + timedelta(seconds=i)).strftime('%m/%d-%H:%M:%S.%f') for i in range(num_samples)]\n",
        "\n",
        "    # Randomly select categorical columns\n",
        "    for col in ['alert', 'classification', 'protocol']:\n",
        "        synthetic_data[col] = np.random.choice(original_df[col].unique(), num_samples)\n",
        "\n",
        "    # Generate priority in range [1,5]\n",
        "    synthetic_data['priority'] = np.random.randint(1, 6, num_samples)\n",
        "\n",
        "    # Generate source and destination IPs\n",
        "    synthetic_data['src_ip'] = [f\"{np.random.randint(1, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}\" for _ in range(num_samples)]\n",
        "    synthetic_data['dst_ip'] = [f\"{np.random.randint(1, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}\" for _ in range(num_samples)]\n",
        "\n",
        "    # Generate valid port numbers\n",
        "    synthetic_data['src_port'] = np.random.randint(0, 65536, num_samples)\n",
        "    synthetic_data['dst_port'] = np.random.randint(0, 65536, num_samples)\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    file_path = '/content/simulated_traffic.csv'\n",
        "    processed_data, numerical_columns, categorical_columns = preprocess_csv_with_dummies(file_path)\n",
        "\n",
        "    # Split data\n",
        "    X = processed_data.values\n",
        "    y = processed_data['classification'].values\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Initialize models\n",
        "    original_dim = X.shape[1]\n",
        "    latent_dim = 32\n",
        "    vae_latent_dim = 16\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Train VAE\n",
        "    vae = VAE(original_dim, vae_latent_dim)\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "    vae.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=50, batch_size=64)\n",
        "\n",
        "    # Train WGAN\n",
        "    generator = Generator(original_dim)\n",
        "    discriminator = Discriminator()\n",
        "    train_wgan(generator, discriminator, X_train, latent_dim)\n",
        "\n",
        "    # Train Classifier\n",
        "    classifier = Classifier(original_dim, num_classes)\n",
        "    history = train_classifier(classifier, X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Evaluate classifier\n",
        "    test_loss, test_accuracy = classifier.evaluate(X_test, y_test)\n",
        "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Save models and preprocessing objects\n",
        "    tf.keras.models.save_model(classifier, 'classifier_model.keras')\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "    joblib.dump(label_encoders, 'label_encoders.pkl')\n",
        "\n",
        "    # Generate synthetic data\n",
        "    num_samples = 1000\n",
        "    z = tf.random.normal((num_samples, latent_dim))\n",
        "    synthetic_data = generate_synthetic_data(num_samples, processed_data)\n",
        "\n",
        "    # Denormalize synthetic data\n",
        "    synthetic_df = denormalize_data(synthetic_data, processed_data, numerical_columns, categorical_columns, label_encoders, scaler)\n",
        "\n",
        "    # Save synthetic data\n",
        "    synthetic_df = pd.DataFrame(synthetic_data, columns=processed_data.columns)\n",
        "    synthetic_df.to_csv('synthetic_traffic.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}