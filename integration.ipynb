{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5ZMO81f/6DhP6PZlXkJ3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikk-17/sys_traffic_gen/blob/main/integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfH3OyzaXP-B",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "# Global variables\n",
        "scaler = None\n",
        "label_encoders = {}\n",
        "\n",
        "def preprocess_csv_with_dummies(file_path, is_training=True):\n",
        "    global scaler, label_encoders\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Detect numerical and categorical columns\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Handle categorical columns with LabelEncoder instead of one-hot encoding\n",
        "    for col in categorical_columns:\n",
        "        if is_training:\n",
        "            label_encoders[col] = LabelEncoder()\n",
        "            df[col] = label_encoders[col].fit_transform(df[col])\n",
        "        else:\n",
        "            df[col] = label_encoders[col].transform(df[col])\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    if numerical_columns:\n",
        "        if is_training:\n",
        "            scaler = MinMaxScaler()\n",
        "            df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "        else:\n",
        "            df[numerical_columns] = scaler.transform(df[numerical_columns])\n",
        "\n",
        "    return df.astype(np.float32), numerical_columns, categorical_columns\n",
        "\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, original_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = self.build_encoder(original_dim, latent_dim)\n",
        "        self.decoder = self.build_decoder(original_dim, latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def build_encoder(self, original_dim, latent_dim):\n",
        "        inputs = tf.keras.layers.Input(shape=(original_dim,))\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return tf.keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def build_decoder(self, original_dim, latent_dim):\n",
        "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(latent_inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        outputs = tf.keras.layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "        return tf.keras.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = tf.keras.losses.mse(inputs, reconstructed)\n",
        "        reconstruction_loss *= tf.cast(tf.shape(inputs)[1], tf.float32)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(data_dim, activation='tanh')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Classifier(tf.keras.Model):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "def gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    alpha = tf.random.uniform([tf.shape(real_samples)[0], 1], 0.0, 1.0)\n",
        "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated)\n",
        "        predictions = discriminator(interpolated)\n",
        "\n",
        "    gradients = tape.gradient(predictions, interpolated)\n",
        "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
        "    gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))\n",
        "    return gradient_penalty\n",
        "\n",
        "def train_wgan(generator, discriminator, data, latent_dim, batch_size=64, epochs=100, n_critic=5, gp_weight=10.0):\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(n_critic):\n",
        "            # Train Discriminator\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                batch_indices = np.random.randint(0, data.shape[0], batch_size)\n",
        "                real_data = data[batch_indices]\n",
        "\n",
        "                z = tf.random.normal((batch_size, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "\n",
        "                real_output = discriminator(real_data, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                gp = gradient_penalty(discriminator, real_data, fake_data)\n",
        "                disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + gp_weight * gp\n",
        "\n",
        "            grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
        "\n",
        "        # Train Generator\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            z = tf.random.normal((batch_size, latent_dim))\n",
        "            fake_data = generator(z, training=True)\n",
        "            fake_output = discriminator(fake_data, training=True)\n",
        "            gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "        grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "            print(f\"Generator Loss: {gen_loss:.4f}\")\n",
        "            print(f\"Discriminator Loss: {disc_loss:.4f}\")\n",
        "\n",
        "def train_classifier(classifier, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
        "    classifier.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = classifier.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def predict_traffic(classifier, data, numerical_columns, categorical_columns):\n",
        "    \"\"\"\n",
        "    Predict traffic classification for new data\n",
        "    \"\"\"\n",
        "    processed_data, _, _ = preprocess_csv_with_dummies(data, is_training=False)\n",
        "    predictions = classifier.predict(processed_data)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Convert numeric predictions back to original labels\n",
        "    original_labels = label_encoders['classification'].inverse_transform(predicted_classes)\n",
        "\n",
        "    return original_labels\n",
        "\n",
        "\n",
        "\n",
        "def denormalize_data(synthetic_data, original_df, numerical_columns, categorical_columns, label_encoders, scaler):\n",
        "    \"\"\"\n",
        "    Denormalize synthetic data to match original data format and ranges\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(synthetic_data, columns=original_df.columns)\n",
        "\n",
        "    # Denormalize numerical columns\n",
        "    if numerical_columns and scaler:\n",
        "        df[numerical_columns] = scaler.inverse_transform(df[numerical_columns])\n",
        "\n",
        "        # Ensure priority remains in the range [1,5]\n",
        "        if 'priority' in df.columns:\n",
        "            df['priority'] = np.round(df['priority']).astype(int).clip(1, 5)\n",
        "\n",
        "        # Ensure port numbers are valid\n",
        "        for col in ['src_port', 'dst_port']:\n",
        "            if col in df.columns:\n",
        "                df[col] = np.round(df[col]).astype(int).clip(0, 65535)\n",
        "\n",
        "    # Convert categorical columns back to original categories\n",
        "    # Exclude timestamp from this process\n",
        "    categorical_columns_excluding_timestamp = [col for col in categorical_columns if col != 'timestamp']\n",
        "    for col in categorical_columns_excluding_timestamp:\n",
        "        if col in label_encoders:\n",
        "            try:\n",
        "                df[col] = label_encoders[col].inverse_transform(df[col].astype(int))\n",
        "            except ValueError:\n",
        "                # If casting to int fails, handle the column differently (e.g., skip or use a custom transformation)\n",
        "                print(f\"Warning: Skipping inverse transform for column '{col}' due to ValueError.\")\n",
        "\n",
        "    # Ensure timestamp format is maintained (if 'timestamp' in df.columns)\n",
        "    if 'timestamp' in df.columns:\n",
        "        #df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%m/%d-%H:%M:%S.%f') #Commented out for now because it may be encoded.\n",
        "\n",
        "        #If the timestamp is already in a string format, just ensure the format remains the same\n",
        "        if df['timestamp'].dtype == object:\n",
        "            pass\n",
        "        else: #If the timestamp is encoded, then we decode it.\n",
        "            df['timestamp'] = label_encoders['timestamp'].inverse_transform(df['timestamp'].astype(int))\n",
        "\n",
        "    # Validate protocol values\n",
        "    if 'protocol' in df.columns:\n",
        "        valid_protocols = ['TCP', 'UDP', 'ICMP']\n",
        "        df['protocol'] = df['protocol'].apply(lambda x: x if x in valid_protocols else np.random.choice(valid_protocols))\n",
        "\n",
        "    # Preserve src_ip and dst_ip as is\n",
        "    if 'src_ip' in df.columns and 'dst_ip' in df.columns:\n",
        "        df['src_ip'] = original_df['src_ip'].values[:len(df)]\n",
        "        df['dst_ip'] = original_df['dst_ip'].values[:len(df)]\n",
        "\n",
        "    # Ensure classification and alert retain valid values\n",
        "    valid_classifications = original_df['classification'].unique().tolist()\n",
        "    if 'classification' in df.columns:\n",
        "        df['classification'] = df['classification'].apply(lambda x: x if x in valid_classifications else np.random.choice(valid_classifications))\n",
        "\n",
        "    valid_alerts = original_df['alert'].unique().tolist()\n",
        "    if 'alert' in df.columns:\n",
        "        df['alert'] = df['alert'].apply(lambda x: x if x in valid_alerts else np.random.choice(valid_alerts))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_synthetic_data(num_samples, original_df):\n",
        "    \"\"\"\n",
        "    Generate synthetic network traffic data similar to the original dataset\n",
        "    \"\"\"\n",
        "    synthetic_data = pd.DataFrame()\n",
        "\n",
        "    # Generate timestamps\n",
        "    start_time = datetime.datetime.strptime(\"02/14-00:00:00.000000\", \"%m/%d-%H:%M:%S.%f\")\n",
        "    synthetic_data['timestamp'] = [(start_time + timedelta(seconds=i)).strftime('%m/%d-%H:%M:%S.%f') for i in range(num_samples)]\n",
        "\n",
        "    # Randomly select categorical columns\n",
        "    for col in ['alert', 'classification', 'protocol']:\n",
        "        synthetic_data[col] = np.random.choice(original_df[col].unique(), num_samples)\n",
        "\n",
        "    # Generate priority in range [1,5]\n",
        "    synthetic_data['priority'] = np.random.randint(1, 6, num_samples)\n",
        "\n",
        "    # Generate source and destination IPs\n",
        "    synthetic_data['src_ip'] = [f\"{np.random.randint(1, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}\" for _ in range(num_samples)]\n",
        "    synthetic_data['dst_ip'] = [f\"{np.random.randint(1, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}.{np.random.randint(0, 256)}\" for _ in range(num_samples)]\n",
        "\n",
        "    # Generate valid port numbers\n",
        "    synthetic_data['src_port'] = np.random.randint(0, 65536, num_samples)\n",
        "    synthetic_data['dst_port'] = np.random.randint(0, 65536, num_samples)\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    file_path = '/content/simulated_traffic.csv'\n",
        "    processed_data, numerical_columns, categorical_columns = preprocess_csv_with_dummies(file_path)\n",
        "\n",
        "    # Split data\n",
        "    X = processed_data.values\n",
        "    y = processed_data['classification'].values\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Initialize models\n",
        "    original_dim = X.shape[1]\n",
        "    latent_dim = 32\n",
        "    vae_latent_dim = 16\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Train VAE\n",
        "    vae = VAE(original_dim, vae_latent_dim)\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "    vae.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=50, batch_size=64)\n",
        "\n",
        "    # Train WGAN\n",
        "    generator = Generator(original_dim)\n",
        "    discriminator = Discriminator()\n",
        "    train_wgan(generator, discriminator, X_train, latent_dim)\n",
        "\n",
        "    # Train Classifier\n",
        "    classifier = Classifier(original_dim, num_classes)\n",
        "    history = train_classifier(classifier, X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Evaluate classifier\n",
        "    test_loss, test_accuracy = classifier.evaluate(X_test, y_test)\n",
        "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Save models and preprocessing objects\n",
        "    tf.keras.models.save_model(classifier, 'classifier_model.keras')\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "    joblib.dump(label_encoders, 'label_encoders.pkl')\n",
        "\n",
        "    # Generate synthetic data\n",
        "    num_samples = 1000\n",
        "    z = tf.random.normal((num_samples, latent_dim))\n",
        "    synthetic_data = generate_synthetic_data(num_samples, processed_data)\n",
        "\n",
        "    # Denormalize synthetic data\n",
        "    synthetic_df = denormalize_data(synthetic_data, processed_data, numerical_columns, categorical_columns, label_encoders, scaler)\n",
        "\n",
        "    # Save synthetic data\n",
        "    synthetic_df = pd.DataFrame(synthetic_data, columns=processed_data.columns)\n",
        "    synthetic_df.to_csv('synthetic_traffic.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# Global variables\n",
        "scaler = None\n",
        "label_encoders = {}\n",
        "\n",
        "def preprocess_csv_with_dummies(file_path, is_training=True):\n",
        "    \"\"\"\n",
        "    Preprocess CSV data with specific features\n",
        "    \"\"\"\n",
        "    global scaler, label_encoders\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Define numerical and categorical columns\n",
        "    numerical_columns = ['src_port', 'dst_port', 'priority']\n",
        "    categorical_columns = ['alert', 'classification', 'protocol']\n",
        "\n",
        "    # Handle categorical columns with LabelEncoder\n",
        "    for col in categorical_columns:\n",
        "        if is_training:\n",
        "            label_encoders[col] = LabelEncoder()\n",
        "            df[col] = label_encoders[col].fit_transform(df[col])\n",
        "        else:\n",
        "            df[col] = label_encoders[col].transform(df[col])\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    if is_training:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "    else:\n",
        "        df[numerical_columns] = scaler.transform(df[numerical_columns])\n",
        "\n",
        "    # Store timestamp and IP addresses separately as they don't need preprocessing\n",
        "    timestamp_data = df['timestamp']\n",
        "    src_ip_data = df['src_ip']\n",
        "    dst_ip_data = df['dst_ip']\n",
        "\n",
        "    # Drop timestamp and IP columns for training\n",
        "    df = df.drop(['timestamp', 'src_ip', 'dst_ip'], axis=1)\n",
        "\n",
        "    return (df.astype(np.float32), numerical_columns, categorical_columns,\n",
        "            timestamp_data, src_ip_data, dst_ip_data)\n",
        "\n",
        "\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, original_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = self.build_encoder(original_dim, latent_dim)\n",
        "        self.decoder = self.build_decoder(original_dim, latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def build_encoder(self, original_dim, latent_dim):\n",
        "        inputs = tf.keras.layers.Input(shape=(original_dim,))\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return tf.keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def build_decoder(self, original_dim, latent_dim):\n",
        "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(latent_inputs)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)\n",
        "        outputs = tf.keras.layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "        return tf.keras.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = tf.keras.losses.mse(inputs, reconstructed)\n",
        "        reconstruction_loss *= tf.cast(tf.shape(inputs)[1], tf.float32)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(data_dim, activation='tanh')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.LayerNormalization(),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "class Classifier(tf.keras.Model):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "def gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    alpha = tf.random.uniform([tf.shape(real_samples)[0], 1], 0.0, 1.0)\n",
        "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated)\n",
        "        predictions = discriminator(interpolated)\n",
        "\n",
        "    gradients = tape.gradient(predictions, interpolated)\n",
        "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
        "    gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))\n",
        "    return gradient_penalty\n",
        "\n",
        "def train_wgan(generator, discriminator, data, latent_dim, batch_size=64, epochs=100, n_critic=5, gp_weight=10.0):\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(n_critic):\n",
        "            # Train Discriminator\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                batch_indices = np.random.randint(0, data.shape[0], batch_size)\n",
        "                real_data = data[batch_indices]\n",
        "\n",
        "                z = tf.random.normal((batch_size, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "\n",
        "                real_output = discriminator(real_data, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                gp = gradient_penalty(discriminator, real_data, fake_data)\n",
        "                disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output) + gp_weight * gp\n",
        "\n",
        "            grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
        "\n",
        "        # Train Generator\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            z = tf.random.normal((batch_size, latent_dim))\n",
        "            fake_data = generator(z, training=True)\n",
        "            fake_output = discriminator(fake_data, training=True)\n",
        "            gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "        grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "            print(f\"Generator Loss: {gen_loss:.4f}\")\n",
        "            print(f\"Discriminator Loss: {disc_loss:.4f}\")\n",
        "\n",
        "def train_classifier(classifier, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
        "    classifier.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = classifier.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def predict_traffic(classifier, data, numerical_columns, categorical_columns):\n",
        "    \"\"\"\n",
        "    Predict traffic classification for new data\n",
        "    \"\"\"\n",
        "    processed_data, _, _ = preprocess_csv_with_dummies(data, is_training=False)\n",
        "    predictions = classifier.predict(processed_data)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Convert numeric predictions back to original labels\n",
        "    original_labels = label_encoders['classification'].inverse_transform(predicted_classes)\n",
        "\n",
        "    return original_labels\n",
        "\n",
        "def denormalize_data(synthetic_data, original_df, numerical_columns, categorical_columns):\n",
        "    \"\"\"\n",
        "    Denormalize synthetic data to match original format\n",
        "    \"\"\"\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(synthetic_data, columns=['alert', 'classification', 'priority',\n",
        "                                             'protocol', 'src_port', 'dst_port'])\n",
        "\n",
        "    # Denormalize numerical columns\n",
        "    if scaler:\n",
        "        df[numerical_columns] = scaler.inverse_transform(df[numerical_columns])\n",
        "\n",
        "        # Round ports and priority to integers and ensure valid ranges\n",
        "        df['src_port'] = np.round(df['src_port']).astype(int).clip(1, 65535)\n",
        "        df['dst_port'] = np.round(df['dst_port']).astype(int).clip(1, 65535)\n",
        "        df['priority'] = np.round(df['priority']).astype(int).clip(1, 5)\n",
        "\n",
        "    # Convert categorical columns back to original labels\n",
        "    for col in categorical_columns:\n",
        "        predicted_classes = np.argmax(df[col].values.reshape(-1, 1), axis=1)\n",
        "        df[col] = label_encoders[col].inverse_transform(predicted_classes)\n",
        "\n",
        "    # Generate timestamps in the correct format\n",
        "    base_timestamp = pd.Timestamp('2025-02-14')\n",
        "    df['timestamp'] = [\n",
        "        (base_timestamp + pd.Timedelta(seconds=i)).strftime('%m/%d-%H:%M:%S.%f')\n",
        "        for i in range(len(df))\n",
        "    ]\n",
        "\n",
        "    # Generate valid IP addresses\n",
        "    df['src_ip'] = df.apply(lambda x: f\"{np.random.randint(1,256)}.{np.random.randint(1,256)}.\"\n",
        "                                     f\"{np.random.randint(1,256)}.{np.random.randint(1,256)}\", axis=1)\n",
        "    df['dst_ip'] = df.apply(lambda x: f\"{np.random.randint(1,256)}.{np.random.randint(1,256)}.\"\n",
        "                                     f\"{np.random.randint(1,256)}.{np.random.randint(1,256)}\", axis=1)\n",
        "\n",
        "    # Reorder columns to match original CSV\n",
        "    df = df[['timestamp', 'alert', 'classification', 'priority', 'protocol',\n",
        "             'src_ip', 'src_port', 'dst_ip', 'dst_port']]\n",
        "\n",
        "    return df\n",
        "\n",
        "# [Previous VAE, Generator, and Discriminator classes remain the same]\n",
        "\n",
        "def generate_synthetic_data(generator, num_samples, latent_dim, original_df):\n",
        "    \"\"\"\n",
        "    Generate synthetic network traffic data\n",
        "    \"\"\"\n",
        "    # Generate raw synthetic data\n",
        "    z = tf.random.normal((num_samples, latent_dim))\n",
        "    synthetic_data = generator(z, training=False).numpy()\n",
        "\n",
        "    # Denormalize the data\n",
        "    numerical_columns = ['src_port', 'dst_port', 'priority']\n",
        "    categorical_columns = ['alert', 'classification', 'protocol']\n",
        "\n",
        "    denormalized_data = denormalize_data(\n",
        "        synthetic_data,\n",
        "        original_df,\n",
        "        numerical_columns,\n",
        "        categorical_columns\n",
        "    )\n",
        "\n",
        "    return denormalized_data\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    file_path = '/content/simulated_traffic.csv'\n",
        "    original_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Process data\n",
        "    (processed_data, numerical_columns, categorical_columns,\n",
        "     timestamp_data, src_ip_data, dst_ip_data) = preprocess_csv_with_dummies(file_path)\n",
        "\n",
        "    # Split data\n",
        "    X = processed_data.values\n",
        "    X_train, X_temp = train_test_split(X, test_size=0.3, random_state=42)\n",
        "    X_val, X_test = train_test_split(X_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Initialize and train models\n",
        "    original_dim = X.shape[1]\n",
        "    latent_dim = 32\n",
        "\n",
        "    # Train VAE\n",
        "    vae = VAE(original_dim, latent_dim)\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "    vae.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=50, batch_size=64)\n",
        "\n",
        "    # Train WGAN\n",
        "    generator = Generator(original_dim)\n",
        "    discriminator = Discriminator()\n",
        "    train_wgan(generator, discriminator, X_train, latent_dim)\n",
        "\n",
        "    # Generate synthetic data\n",
        "    num_samples = 1000\n",
        "    synthetic_df = generate_synthetic_data(generator, num_samples, latent_dim, original_df)\n",
        "\n",
        "    # Save synthetic data\n",
        "    synthetic_df.to_csv('trial.csv', index=False)\n",
        "\n",
        "    # Print sample comparison\n",
        "    print(\"\\nOriginal Data Sample:\")\n",
        "    print(original_df.head())\n",
        "    print(\"\\nSynthetic Data Sample:\")\n",
        "    print(synthetic_df.head())\n",
        "\n",
        "    # Verify data structure\n",
        "    print(\"\\nColumn names match:\",\n",
        "          all(synthetic_df.columns == original_df.columns))\n",
        "    print(\"\\nData types:\")\n",
        "    for col in synthetic_df.columns:\n",
        "        print(f\"{col}: {synthetic_df[col].dtype}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHWnIhseo35S",
        "outputId": "c1776e7e-2ecf-42f7-8d90-69f925d953f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 16.1511 - val_loss: 14.1571\n",
            "Epoch 2/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.4929 - val_loss: 14.1479\n",
            "Epoch 3/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1910 - val_loss: 14.1437\n",
            "Epoch 4/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1533 - val_loss: 14.1413\n",
            "Epoch 5/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2639 - val_loss: 14.1439\n",
            "Epoch 6/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.3032 - val_loss: 14.1460\n",
            "Epoch 7/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2471 - val_loss: 14.1442\n",
            "Epoch 8/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.3863 - val_loss: 14.1434\n",
            "Epoch 9/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.0970 - val_loss: 14.1434\n",
            "Epoch 10/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1367 - val_loss: 14.1411\n",
            "Epoch 11/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.9701 - val_loss: 14.1407\n",
            "Epoch 12/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.4661 - val_loss: 14.1379\n",
            "Epoch 13/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.4241 - val_loss: 14.1437\n",
            "Epoch 14/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1755 - val_loss: 14.1367\n",
            "Epoch 15/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.4607 - val_loss: 14.1402\n",
            "Epoch 16/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.4791 - val_loss: 14.1479\n",
            "Epoch 17/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.5161 - val_loss: 14.1426\n",
            "Epoch 18/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2342 - val_loss: 14.1464\n",
            "Epoch 19/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.3176 - val_loss: 14.1406\n",
            "Epoch 20/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2183 - val_loss: 14.1405\n",
            "Epoch 21/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2043 - val_loss: 14.1402\n",
            "Epoch 22/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2376 - val_loss: 14.1384\n",
            "Epoch 23/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1913 - val_loss: 14.1403\n",
            "Epoch 24/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.5406 - val_loss: 14.1480\n",
            "Epoch 25/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.9892 - val_loss: 14.1416\n",
            "Epoch 26/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 14.2426 - val_loss: 14.1411\n",
            "Epoch 27/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.1749 - val_loss: 14.1418\n",
            "Epoch 28/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.4336 - val_loss: 14.1380\n",
            "Epoch 29/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.8609 - val_loss: 14.1411\n",
            "Epoch 30/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2803 - val_loss: 14.1386\n",
            "Epoch 31/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.1680 - val_loss: 14.1372\n",
            "Epoch 32/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.7847 - val_loss: 14.1386\n",
            "Epoch 33/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.5173 - val_loss: 14.1378\n",
            "Epoch 34/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2268 - val_loss: 14.1409\n",
            "Epoch 35/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.3576 - val_loss: 14.1435\n",
            "Epoch 36/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.9809 - val_loss: 14.1404\n",
            "Epoch 37/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.3235 - val_loss: 14.1436\n",
            "Epoch 38/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.3958 - val_loss: 14.1383\n",
            "Epoch 39/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.0199 - val_loss: 14.1415\n",
            "Epoch 40/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.4542 - val_loss: 14.1413\n",
            "Epoch 41/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1994 - val_loss: 14.1393\n",
            "Epoch 42/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.1641 - val_loss: 14.1409\n",
            "Epoch 43/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.0498 - val_loss: 14.1383\n",
            "Epoch 44/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.0780 - val_loss: 14.1379\n",
            "Epoch 45/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.3661 - val_loss: 14.1388\n",
            "Epoch 46/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1382 - val_loss: 14.1387\n",
            "Epoch 47/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1214 - val_loss: 14.1405\n",
            "Epoch 48/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.9342 - val_loss: 14.1409\n",
            "Epoch 49/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.6571 - val_loss: 14.1419\n",
            "Epoch 50/50\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.1459 - val_loss: 14.1419\n",
            "Epoch 10/100\n",
            "Generator Loss: -0.2323\n",
            "Discriminator Loss: 9.2148\n",
            "Epoch 20/100\n",
            "Generator Loss: -0.4798\n",
            "Discriminator Loss: 1.8371\n",
            "Epoch 30/100\n",
            "Generator Loss: -0.2968\n",
            "Discriminator Loss: 0.6818\n",
            "Epoch 40/100\n",
            "Generator Loss: 0.4135\n",
            "Discriminator Loss: 0.4796\n",
            "Epoch 50/100\n",
            "Generator Loss: 0.9030\n",
            "Discriminator Loss: -0.4218\n",
            "Epoch 60/100\n",
            "Generator Loss: 2.3836\n",
            "Discriminator Loss: -1.2398\n",
            "Epoch 70/100\n",
            "Generator Loss: 3.0471\n",
            "Discriminator Loss: -1.9134\n",
            "Epoch 80/100\n",
            "Generator Loss: 4.1328\n",
            "Discriminator Loss: -2.1888\n",
            "Epoch 90/100\n",
            "Generator Loss: 5.1605\n",
            "Discriminator Loss: -2.3443\n",
            "Epoch 100/100\n",
            "Generator Loss: 6.0309\n",
            "Discriminator Loss: -2.7664\n",
            "\n",
            "Original Data Sample:\n",
            "               timestamp                     alert  \\\n",
            "0  02/14-00:00:00.000000   SNMP AgentX/tcp request   \n",
            "1  02/14-00:00:01.000000                 ICMP PING   \n",
            "2  02/14-00:00:02.000000  ICMP PING undefined code   \n",
            "3  02/14-00:00:03.000000         ICMP PING BSDtype   \n",
            "4  02/14-00:00:04.000000            SCAN nmap XMAS   \n",
            "\n",
            "               classification  priority protocol         src_ip  src_port  \\\n",
            "0               Misc activity         2      TCP  155.48.45.107     60577   \n",
            "1  Attempted Information Leak         3      TCP  162.146.52.45     33549   \n",
            "2               Misc activity         3      UDP   63.234.36.85     13503   \n",
            "3               Misc activity         2      UDP    181.46.31.4      5522   \n",
            "4               Misc activity         2      UDP   161.66.55.32     11732   \n",
            "\n",
            "           dst_ip  dst_port  \n",
            "0     98.78.164.1     11898  \n",
            "1   24.132.14.194     26018  \n",
            "2  66.250.157.165     36009  \n",
            "3   134.105.168.9     34646  \n",
            "4  103.212.58.250     60567  \n",
            "\n",
            "Synthetic Data Sample:\n",
            "               timestamp                           alert  \\\n",
            "0  02/14-00:00:00.000000  DDOS mstream client to handler   \n",
            "1  02/14-00:00:01.000000  DDOS mstream client to handler   \n",
            "2  02/14-00:00:02.000000  DDOS mstream client to handler   \n",
            "3  02/14-00:00:03.000000  DDOS mstream client to handler   \n",
            "4  02/14-00:00:04.000000  DDOS mstream client to handler   \n",
            "\n",
            "                classification  priority protocol           src_ip  src_port  \\\n",
            "0  Attempted Denial of Service         2     ICMP      89.41.13.69     56096   \n",
            "1  Attempted Denial of Service         3     ICMP  250.213.202.252     56810   \n",
            "2  Attempted Denial of Service         3     ICMP      247.33.75.3     57160   \n",
            "3  Attempted Denial of Service         3     ICMP    26.41.146.156     57553   \n",
            "4  Attempted Denial of Service         3     ICMP     15.103.67.96     60287   \n",
            "\n",
            "           dst_ip  dst_port  \n",
            "0  64.182.105.121     21591  \n",
            "1   220.14.243.18     45066  \n",
            "2   96.215.125.27     19806  \n",
            "3  145.14.145.141     37276  \n",
            "4   24.222.161.15     39431  \n",
            "\n",
            "Column names match: True\n",
            "\n",
            "Data types:\n",
            "timestamp: object\n",
            "alert: object\n",
            "classification: object\n",
            "priority: int64\n",
            "protocol: object\n",
            "src_ip: object\n",
            "src_port: int64\n",
            "dst_ip: object\n",
            "dst_port: int64\n"
          ]
        }
      ]
    }
  ]
}