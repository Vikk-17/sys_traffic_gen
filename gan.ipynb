{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data.columns = data.columns.str.strip() # Removing whitespaces\n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    labels = data['Label'].copy()\n",
    "    features = data.drop(columns=['Label'])\n",
    "\n",
    "    # Encode labels to binary (0 for normal, 1 for attack)\n",
    "    labels = (labels != 'BENIGN').astype(int)\n",
    "\n",
    "      # Replace infinite values with NaN\n",
    "    features = features.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Fill NaN values using suitable strategy (e.g., mean or median)\n",
    "    for col in features.columns:\n",
    "      features[col] = features[col].fillna(features[col].mean())\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    return features, labels, scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(tf.keras.Model):\n",
    "  def __init__(self, noise_dim, feature_dim):\n",
    "    super(Generator, self).__init__()\n",
    "    # Creating layers\n",
    "    self.dense1 = tf.keras.layers.Dense(units=128, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(units=256, activation='relu')\n",
    "    self.dense3 = tf.keras.layers.Dense(feature_dim, activation='tanh')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense1(inputs)\n",
    "    x = self.dense2(x)\n",
    "    return self.dense3(x)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(units=256, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(units=128, activation='relu')\n",
    "    self.dense3 = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense1(inputs)\n",
    "    x = self.dense2(x)\n",
    "    return self.dense3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "def train_gen(generator, discriminator, features, epochs, batch_size, noise_dim):\n",
    "  cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "  gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "  disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(real_data):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    generated_data = generator(noise)\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "      real_output = discriminator(real_data)\n",
    "      fake_output = discriminator(generated_data)\n",
    "      disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "      generated_data = generator(noise)\n",
    "      fake_output = discriminator(generated_data)\n",
    "      gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(0, len(features), batch_size):\n",
    "      batch_data = features[i:i+batch_size]\n",
    "      gen_loss, disc_loss = train_step(batch_data)\n",
    "    print(f\"Epoch {epoch + 1}, Generator loss: {gen_loss.numpy()}, Discrimintor loss: {disc_loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic traffic\n",
    "def generate_synthetic_traffic(generator, num_samples, noise_dim):\n",
    "  noise = tf.random.uniform([num_samples, noise_dim], maxval=1, minval=-1)\n",
    "  # set training false for inference\n",
    "  synthetic_data = generator(noise, training=False)\n",
    "  return synthetic_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  data = load_data(\"/content/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "  features, labels, scaler = preprocess_data(data)\n",
    "\n",
    "  noise_dim = 100\n",
    "  feature_dim = features.shape[1]\n",
    "\n",
    "  generator = Generator(noise_dim, feature_dim)\n",
    "  discriminator = Discriminator()\n",
    "\n",
    "  train_gen(generator, discriminator, features, epochs=100, batch_size=32, noise_dim=noise_dim)\n",
    "\n",
    "  # Generate synthetic traffic after training\n",
    "  num_samples = 1000 # Number of synthetic samples to generate\n",
    "  synthetic_traffic = generate_synthetic_traffic(generator, num_samples, noise_dim)\n",
    "\n",
    "  # Inverse transform to original scale\n",
    "  synthetic_traffic = scaler.inverse_transform(synthetic_traffic)\n",
    "\n",
    "  # Save to a csv file\n",
    "  synthetic_df = pd.DataFrame(synthetic_traffic, columns=data.drop(columns=['Label']).columns)\n",
    "  synthetic_df.to_csv('synthetic_traffic_ddos.csv', index=False)\n",
    "  print(\"Synthetic traffic saved to 'synthetic_traffic_ddos.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
