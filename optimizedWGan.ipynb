{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikk-17/sys_traffic_gen/blob/main/optimizedWGan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58C3oYpT_G7v"
      },
      "source": [
        "# TODOS:\n",
        "-   Try with diff params for optimization\n",
        "-   Optimizers such as ADAM, RMSProp, learning_rate\n",
        "-   Non-linear actiavations\n",
        "-   Use less features\n",
        "-   Test the model with X_test\n",
        "-   Cross Validation\n",
        "-   Save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8ndp9nx77i0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8PXJACd8sgC"
      },
      "outputs": [],
      "source": [
        "# Load multiple datasets\n",
        "def load_datasets(file_paths):\n",
        "    \"\"\"\n",
        "    Takes a list of file paths and returns a combined data frame.\n",
        "    :params: list of file paths\n",
        "    :return: combined data frame\n",
        "    \"\"\"\n",
        "    df_list = []\n",
        "    for file_path in file_paths:\n",
        "        df = pd.read_csv(file_path)\n",
        "        df = df.dropna()\n",
        "        df.columns = df.columns.str.strip()\n",
        "        df_list.append(df)\n",
        "    combined_df = pd.concat(df_list, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "def process_data(dataFrame):\n",
        "    \"\"\"\n",
        "    Takes data frame and returns features, labels.\n",
        "    :params: pandas data frame\n",
        "    :return: features: numpy array, labels: numpy array\n",
        "    \"\"\"\n",
        "    labels = dataFrame['Label'].copy()\n",
        "    features = dataFrame.drop(columns=['Label'])\n",
        "\n",
        "    # Encode labels to binary (0 for normal, 1 for attack)\n",
        "    # labels = (labels != 'BENIGN').astype(int)\n",
        "    # Encode labels to integers\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Replce any infinity values with NaN\n",
        "    features = features.replace(to_replace=[-np.inf, np.inf], value=np.nan)\n",
        "\n",
        "    # Fill NaN values using suitable strategy (e.g., mean or median)\n",
        "    features = features.fillna(features.mean())\n",
        "\n",
        "    # Normalize features to a specific range [0, 1] both including\n",
        "    # x_scaled = (x-x_min) / (x_max-x_min)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # (153000, 78), (33000, 78), (153000,), (33000,)\n",
        "    return X_train, X_test, y_train, y_test, scaler, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-coUnfvL5lV1"
      },
      "outputs": [],
      "source": [
        "# VAE Encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(units=128, activation='relu') # non linear activation [0, 0.5]\n",
        "        self.dense2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
        "\n",
        "        self.latent_mean = tf.keras.layers.Dense(latent_dim) # linear activation\n",
        "        self.latent_log_var = tf.keras.layers.Dense(latent_dim)\n",
        "\n",
        "\n",
        "    # returns the latent space (latent_mean, latent_log_var)\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        mean = self.latent_mean(x)\n",
        "        log_var = self.latent_log_var(x)\n",
        "        return mean, log_var\n",
        "\n",
        "\n",
        "# Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, feature_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(units=64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(units=128, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(units=feature_dim, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        reconstructed = self.output_layer(x)\n",
        "        return reconstructed\n",
        "\n",
        "\n",
        "# Variational Autoencoder\n",
        "class VariationalAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, latent_dim, feature_dim):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(feature_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        z = u + sigma * epsilon\n",
        "        where u is the mean and sigma is the log variance\n",
        "        and epsilon is any value between -1 and 1\n",
        "        \"\"\"\n",
        "        mean, log_var = self.encoder(inputs)\n",
        "        epsilon = tf.random.normal(shape=tf.shape(mean))\n",
        "        z = mean + tf.exp(log_var * 0.5) * epsilon\n",
        "        reconstructed = self.decoder(z)\n",
        "        return reconstructed, mean, log_var\n",
        "\n",
        "\n",
        "def pretrain_vae(vae, data, epochs):\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstructed, mean, log_var = vae(inputs)\n",
        "            reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.cast(inputs, tf.float32), reconstructed))\n",
        "            kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
        "            loss = reconstruction_loss + kl_loss\n",
        "        gradients = tape.gradient(loss, vae.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
        "\n",
        "        # reconstruction accuracy\n",
        "        recon_accuracy = tf.reduce_mean(tf.cast(tf.abs(tf.cast(inputs, tf.float32) - reconstructed) < 0.1, tf.float32)) * 100\n",
        "        return loss, recon_accuracy\n",
        "\n",
        "    # for epoch in range(epochs):\n",
        "    #   for i in range(0, len(data), 64):\n",
        "    #     batch_data = data[i:i+64]\n",
        "    #     loss = train_step(batch_data)\n",
        "    #   print(f\"Epoch {epoch+1}, VAE Loss: {loss.numpy()}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(data).batch(64)\n",
        "\n",
        "\n",
        "        for batch in dataset:\n",
        "            loss, accuracy = train_step(batch)\n",
        "            epoch_loss += accuracy / len(dataset)\n",
        "            epoch_accuracy += accuracy / len(dataset)\n",
        "        print(f\"Epoch {epoch + 1}, VAE Loss: {loss.numpy()}, Reconstructed Accuracy: {epoch_accuracy.numpy()}%\")\n",
        "\n",
        "\n",
        "def test_vae(vae, X_test, scaler):\n",
        "    # scaler = MinMaxScaler()\n",
        "    X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "    reconstructed, _, _  = vae(X_test_normalized)\n",
        "\n",
        "    # compute reconsturction loss\n",
        "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(X_test_normalized, reconstructed))\n",
        "\n",
        "    recon_accuracy = tf.reduce_mean(tf.cast(tf.abs(X_test_normalized - reconstructed)))\n",
        "\n",
        "    print(f\"VAE Reconstruction Loss: {reconstruction_loss.numpy()}\")\n",
        "    print(f\"VAE Reconstruction Accuracy: {recon_accuracy.numpy()}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqKQGBsUqMX9"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "class WGANGenerator(tf.keras.Model):\n",
        "    def __init__(self, latent_dim, feature_dim):\n",
        "        super(WGANGenerator, self).__init__()\n",
        "        self.vae_decoder = Decoder(feature_dim)\n",
        "\n",
        "    def call(self, z):\n",
        "        return self.vae_decoder(z)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(units=256, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(units=128, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(units=1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "\n",
        "def train_wgan(generator, discriminator, real_data, epochs, batch_size, latent_dim, clip_value=0.01):\n",
        "    gen_optimizer = tf.keras.optimizers.RMSprop(learning_rate=5e-5)\n",
        "    disc_optimizer = tf.keras.optimizers.RMSprop(learning_rate=5e-5)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(real_data):\n",
        "        noise = tf.random.normal([batch_size, latent_dim])\n",
        "\n",
        "        # Train discriminator\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            fake_data = generator(noise)\n",
        "            real_output = discriminator(real_data)\n",
        "            fake_output = discriminator(fake_data)\n",
        "            disc_loss = -(tf.reduce_mean(real_output) - tf.reduce_mean(fake_output))\n",
        "\n",
        "        disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "        disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "        # Clip discriminator weights\n",
        "        for var in discriminator.trainable_variables:\n",
        "            var.assign(tf.clip_by_value(var, -clip_value, clip_value))\n",
        "\n",
        "        # Train generator\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            fake_data = generator(noise)\n",
        "            fake_output = discriminator(fake_data)\n",
        "            gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "        gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gen_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "\n",
        "        # Compute generator accuracy (percentage of fake data classified as real)\n",
        "        fake_predictions = tf.sigmoid(fake_output) > 0.5\n",
        "        gen_accuracy = tf.reduce_mean(tf.cast(fake_predictions, tf.float32)) * 100\n",
        "\n",
        "        return gen_loss, disc_loss, gen_accuracy\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_gen_loss = 0\n",
        "        epoch_disc_loss = 0\n",
        "        epoch_gen_accuracy = 0\n",
        "        num_batches = len(real_data) // batch_size\n",
        "\n",
        "        for i in range(0, len(real_data), batch_size):\n",
        "            real_batch = real_data[i:i+batch_size]\n",
        "            gen_loss, disc_loss, gen_accuracy = train_step(real_batch)\n",
        "            epoch_gen_loss += gen_loss / num_batches\n",
        "            epoch_disc_loss += disc_loss / num_batches\n",
        "            epoch_gen_accuracy += gen_accuracy / num_batches\n",
        "        print(f\"Epoch {epoch+1}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}, Generator Accuracy: {epoch_gen_accuracy.numpy()}%\")\n",
        "\n",
        "\n",
        "def test_wgan(generator, discriminator, X_test, latent_dim, scaler, batch_size=64):\n",
        "    # Normalize the test_data\n",
        "    real_data = scaler.transform(X_test)\n",
        "\n",
        "    # Generate the fake_data using the generator\n",
        "    noise = tf.random.normal([len(X_test), latent_dim])\n",
        "    fake_data = generator(noise)\n",
        "\n",
        "    # Evaluate the discriminator on real and fake data\n",
        "    real_output = discriminator(real_data)\n",
        "    fake_output = discriminator(fake_data)\n",
        "\n",
        "    # Calculate discriminator accuracy\n",
        "    real_accuracy = tf.reduce_mean(tf.cast(real_output > 0, tf.float32)) * 100\n",
        "    fake_accuracy = tf.reduce_mean(tf.cast(fake_output < 0, tf.float32)) * 100\n",
        "\n",
        "    print(f\"Discriminator Accuracy on Real Data: {real_accuracy.numpy()}%\")\n",
        "    print(f\"Discriminator Accuracy on Fake Data: {fake_accuracy.numpy()}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined VAE + WGAN Model\n",
        "class VAEWGAN(tf.keras.Model):\n",
        "    def __init__(self, vae, generator, discriminator):\n",
        "        super(VAEWGAN, self).__init__()\n",
        "        self.vae = vae\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # VAE part\n",
        "        reconstructed, mean, log_var = self.vae(inputs)\n",
        "\n",
        "        # WGAN Generator part\n",
        "        noise = tf.random.normal([inputs.shape[0], self.generator.input_shape[-1]])\n",
        "        synthetic_data = self.generator(noise)\n",
        "\n",
        "        return {\n",
        "            'vae_reconstruction': reconstructed,\n",
        "            'vae_mean': mean,\n",
        "            'vae_log_var': log_var,\n",
        "            'synthetic_data': synthetic_data\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Q8iunAIgRt-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3REXWyRlM-W"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_traffic(combine_df, generator, scaler, label_encoder, latent_dim, num_samples=1000):\n",
        "    noise = tf.random.normal([num_samples, latent_dim])\n",
        "    synthetic_data = generator(noise).numpy()\n",
        "    synthetic_data = scaler.inverse_transform(synthetic_data)\n",
        "\n",
        "    # Get original column names (excluding 'Label')\n",
        "    original_columns = combine_df.drop(columns=[\"Label\"]).columns\n",
        "\n",
        "    # Create DataFrame with original column names\n",
        "    synthetic_df = pd.DataFrame(synthetic_data, columns=original_columns)\n",
        "\n",
        "    # Convert specific fields to integers and apply valid ranges\n",
        "    # synthetic_df['Total Forwarded Packets'] = synthetic_df['Total Forwarded Packets'].astype(int)\n",
        "    synthetic_df['Destination Port'] = synthetic_df['Destination Port'].astype(int).clip(0, 65535)\n",
        "\n",
        "    # Assign labels based on noise input\n",
        "    attack_types = label_encoder.classes_\n",
        "    synthetic_labels = []\n",
        "    for _ in range(num_samples):\n",
        "        attack_type = np.random.choice(attack_types)\n",
        "        synthetic_labels.append(attack_type)\n",
        "    synthetic_df['Label'] = synthetic_labels\n",
        "\n",
        "    synthetic_df.to_csv('synthetic_traffic.csv', index=False)\n",
        "    print(\"Synthetic traffic saved to 'synthetic_traffic.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9OxxkrnGOUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea117726-440b-4486-b031-f100dc26a093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, VAE Loss: 0.154277965426445, Reconstructed Accuracy: 89.58180236816406%\n",
            "Epoch 2, VAE Loss: 0.15425603091716766, Reconstructed Accuracy: 89.75859069824219%\n",
            "Epoch 3, VAE Loss: 0.15423071384429932, Reconstructed Accuracy: 89.78211975097656%\n",
            "Epoch 4, VAE Loss: 0.15419530868530273, Reconstructed Accuracy: 89.79608154296875%\n",
            "Epoch 5, VAE Loss: 0.1541604846715927, Reconstructed Accuracy: 89.803955078125%\n",
            "Epoch 6, VAE Loss: 0.15413358807563782, Reconstructed Accuracy: 89.81421661376953%\n",
            "Epoch 7, VAE Loss: 0.15411099791526794, Reconstructed Accuracy: 89.81996154785156%\n",
            "Epoch 8, VAE Loss: 0.15410666167736053, Reconstructed Accuracy: 89.82266998291016%\n",
            "Epoch 9, VAE Loss: 0.15410347282886505, Reconstructed Accuracy: 89.83076477050781%\n",
            "Epoch 10, VAE Loss: 0.15409192442893982, Reconstructed Accuracy: 89.83351135253906%\n",
            "Epoch 11, VAE Loss: 0.15409578382968903, Reconstructed Accuracy: 89.83770751953125%\n",
            "Epoch 12, VAE Loss: 0.15409541130065918, Reconstructed Accuracy: 89.84056854248047%\n",
            "Epoch 13, VAE Loss: 0.1540854424238205, Reconstructed Accuracy: 89.84114837646484%\n",
            "Epoch 14, VAE Loss: 0.15407805144786835, Reconstructed Accuracy: 89.84162139892578%\n",
            "Epoch 15, VAE Loss: 0.15407951176166534, Reconstructed Accuracy: 89.84212493896484%\n",
            "Epoch 16, VAE Loss: 0.15407247841358185, Reconstructed Accuracy: 89.84201049804688%\n",
            "Epoch 17, VAE Loss: 0.15406683087348938, Reconstructed Accuracy: 89.84219360351562%\n",
            "Epoch 18, VAE Loss: 0.15406286716461182, Reconstructed Accuracy: 89.84201049804688%\n",
            "Epoch 19, VAE Loss: 0.15406134724617004, Reconstructed Accuracy: 89.84163665771484%\n",
            "Epoch 20, VAE Loss: 0.1540650874376297, Reconstructed Accuracy: 89.8423080444336%\n",
            "Epoch 1, Generator Loss: -0.003531759139150381, Discriminator Loss: -0.0005825916305184364, Generator Accuracy: 95.76776123046875%\n",
            "Epoch 2, Generator Loss: 0.02039274200797081, Discriminator Loss: -0.004636894911527634, Generator Accuracy: 0.6815513968467712%\n",
            "Epoch 3, Generator Loss: 0.014330723322927952, Discriminator Loss: -0.0018905717879533768, Generator Accuracy: 0.0%\n",
            "Epoch 4, Generator Loss: 0.007024475373327732, Discriminator Loss: -0.0025018523447215557, Generator Accuracy: 0.0048597571440041065%\n",
            "Epoch 5, Generator Loss: 0.007561210542917252, Discriminator Loss: -0.0019469070248305798, Generator Accuracy: 0.003004213562235236%\n",
            "Epoch 6, Generator Loss: 0.009125838056206703, Discriminator Loss: -0.001885371282696724, Generator Accuracy: 0.004241242539137602%\n",
            "Epoch 7, Generator Loss: 0.010553850792348385, Discriminator Loss: -0.00197397917509079, Generator Accuracy: 0.0011928494786843657%\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    file_paths = [\n",
        "        \"/content/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
        "        \"/content/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "        \"/content/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "        \"/content/Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "        \"/content/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "        \"/content/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
        "        \"/content/Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
        "        \"/content/Wednesday-workingHours.pcap_ISCX.csv\",\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "    data = load_datasets(file_paths)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test, scaler, label_encoder = process_data(data)\n",
        "\n",
        "\n",
        "    latent_dim = 10\n",
        "    feature_dim = X_train.shape[1] # 78\n",
        "\n",
        "    vae = VariationalAutoencoder(latent_dim, feature_dim)\n",
        "    pretrain_vae(vae, X_train, epochs=20)\n",
        "\n",
        "    # Test vae\n",
        "    # test_vae(vae, X_test, scaler)\n",
        "\n",
        "    generator = WGANGenerator(latent_dim, feature_dim)\n",
        "    discriminator = Discriminator()\n",
        "\n",
        "    train_wgan(generator, discriminator, X_train, epochs=50, batch_size=64, latent_dim=latent_dim)\n",
        "\n",
        "\n",
        "    # Create combined model\n",
        "    combined_model = VAEWGAN(vae, generator, discriminator)\n",
        "\n",
        "    # Save the combined model\n",
        "    combined_model.save('vae_wgan_model.keras')\n",
        "    print(\"Combined model saved as 'vae_wgan_model.keras'.\")\n",
        "\n",
        "\n",
        "    # Test WGAN\n",
        "    # test_wgan(generator, discriminator, X_test, scaler, latent_dim)\n",
        "\n",
        "    # Generate synthetic traffic\n",
        "    # noise = tf.random.normal([1000, latent_dim])\n",
        "    # synthetic_data = generator(noise).numpy()\n",
        "\n",
        "    # synthetic_data = scaler.inverse_transform(synthetic_data)\n",
        "    # pd.DataFrame(synthetic_data, columns=data.drop(columns=[\"Label\"]).columns).to_csv(\"Synthetic_traffic.csv\", index=False)\n",
        "\n",
        "    # print(\"Synthetic traffic saved to 'synthetic_traffic.csv'\")\n",
        "\n",
        "    # generate_synthetic_traffic(data, generator, scaler, label_encoder, latent_dim, num_samples=1000)\n",
        "\n",
        "\n",
        "    # Save the model for further use\n",
        "    # synthetic_data.save('synthetic_traffic.keras')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMaDsZopX4tQh8oknJ225qf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}