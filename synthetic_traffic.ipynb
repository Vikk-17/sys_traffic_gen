{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOISgpYEWsiY6sMtJukwrv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikk-17/sys_traffic_gen/blob/main/synthetic_traffic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Preprocessing function with get_dummies\n",
        "scaler = None  # Global scaler to maintain consistency\n",
        "\n",
        "def preprocess_csv_with_dummies(file_path):\n",
        "    global scaler\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Detect numerical and categorical columns\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    if numerical_columns:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "    # One-hot encode categorical columns using pd.get_dummies\n",
        "    if categorical_columns:\n",
        "        df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "    # Convert the DataFrame to float32 for compatibility with TensorFlow\n",
        "    return df.astype(np.float32), numerical_columns, categorical_columns\n",
        "\n",
        "# Load and preprocess data\n",
        "file_path = '/content/synthetic_cicids_dataset_detailed.csv'  # Replace with your CSV file path\n",
        "processed_data, numerical_columns, categorical_columns = preprocess_csv_with_dummies(file_path)\n",
        "\n",
        "# Convert to NumPy array\n",
        "data = processed_data.values\n",
        "\n",
        "X_train, X_temp = train_test_split(data, test_size=0.4, random_state=42)\n",
        "X_valid, X_test = train_test_split(data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Define Variational Autoencoder components\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(Model):\n",
        "    def __init__(self, original_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = self.build_encoder(original_dim, latent_dim)\n",
        "        self.decoder = self.build_decoder(original_dim, latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def build_encoder(self, original_dim, latent_dim):\n",
        "        inputs = Input(shape=(original_dim,))\n",
        "        x = Dense(128, activation=\"relu\")(inputs)\n",
        "        x = Dense(64, activation=\"relu\")(x)\n",
        "        z_mean = Dense(latent_dim)(x)\n",
        "        z_log_var = Dense(latent_dim)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def build_decoder(self, original_dim, latent_dim):\n",
        "        latent_inputs = Input(shape=(latent_dim,))\n",
        "        x = Dense(64, activation=\"relu\")(latent_inputs)\n",
        "        x = Dense(128, activation=\"relu\")(x)\n",
        "        outputs = Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "        return Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = tf.keras.losses.mse(inputs, reconstructed)\n",
        "        reconstruction_loss *= tf.cast(tf.shape(inputs)[1], tf.float32)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "# Define WGAN components\n",
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.dense1 = Dense(128, activation='relu')\n",
        "        self.dense2 = Dense(256, activation='relu')\n",
        "        self.dense3 = Dense(data_dim, activation='tanh')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "class Discriminator(Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.dense1 = Dense(256, activation='relu')\n",
        "        self.dense2 = Dense(128, activation='relu')\n",
        "        self.dense3 = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "# WGAN training loop\n",
        "def train_wgan(generator, discriminator, data, latent_dim, batch_size=64, epochs=100, learning_rate=0.0001):\n",
        "    # Optimizers\n",
        "    gen_optimizer = Adam(learning_rate)\n",
        "    disc_optimizer = Adam(learning_rate)\n",
        "\n",
        "    # Training step\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, data.shape[0], batch_size):\n",
        "            real_data = data[i:i + batch_size]\n",
        "            batch_size_real = real_data.shape[0]\n",
        "\n",
        "            # Train Discriminator\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                z = tf.random.normal((batch_size_real, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "                real_output = discriminator(real_data, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "            grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
        "\n",
        "            # Train Generator\n",
        "            with tf.GradientTape() as gen_tape:\n",
        "                z = tf.random.normal((batch_size_real, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "            grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "            gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}\")\n",
        "\n",
        "# Initialize generator, discriminator, and VAE\n",
        "original_dim = data.shape[1]\n",
        "latent_dim = 10\n",
        "vae_latent_dim = 5\n",
        "\n",
        "vae = VAE(original_dim, vae_latent_dim)\n",
        "vae.compile(optimizer=Adam(learning_rate=0.0001), metrics=[tf.keras.losses.MeanSquaredError()])\n",
        "vae.fit(X_train, X_train, validation_data=(X_valid, X_valid), epochs=50, batch_size=64)\n",
        "\n",
        "generator = Generator(original_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Train WGAN\n",
        "train_wgan(generator, discriminator, X_train, latent_dim)\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_synthetic_data(generator, num_samples, latent_dim):\n",
        "    z = tf.random.normal((num_samples, latent_dim))\n",
        "    synthetic_data = generator(z, training=False).numpy()\n",
        "    return synthetic_data\n",
        "\n",
        "synthetic_samples = generate_synthetic_data(generator, num_samples=1000, latent_dim=latent_dim)\n",
        "\n",
        "# Transform synthetic data back to match original input\n",
        "\n",
        "def postprocess_synthetic_data(synthetic_data, original_df, numerical_columns, categorical_columns):\n",
        "    global scaler\n",
        "    df = pd.DataFrame(synthetic_data, columns=original_df.columns)\n",
        "\n",
        "    # Denormalize numerical columns\n",
        "    if numerical_columns and scaler:\n",
        "        df[numerical_columns] = scaler.inverse_transform(df[numerical_columns])\n",
        "\n",
        "        # Clip and round ports to valid range\n",
        "        if 'Source Port' in numerical_columns:\n",
        "            df['Source Port'] = df['Source Port'].clip(0, 65535).round().astype(int)\n",
        "        if 'Destination Port' in numerical_columns:\n",
        "            df['Destination Port'] = df['Destination Port'].clip(0, 65535).round().astype(int)\n",
        "\n",
        "    # Convert one-hot encoded columns back to original categories\n",
        "    for cat_col in categorical_columns:\n",
        "        cat_prefix = [col for col in df.columns if col.startswith(cat_col + '_')]\n",
        "        if cat_prefix:\n",
        "            df[cat_col] = df[cat_prefix].idxmax(axis=1).apply(lambda x: x.split('_', 1)[-1])\n",
        "            df = df.drop(columns=cat_prefix)\n",
        "\n",
        "    # Mimic original labels based on given traffic types (Benign or Malicious)\n",
        "    # if 'Label' in df.columns and 'Attack_Type' in df.columns:\n",
        "    #     df['Label'] = df['Attack_Type'].apply(lambda x: 'Benign' if x.lower() == 'normal' else 'Malicious')\n",
        "\n",
        "    if 'Attack Type' in df.columns:\n",
        "        df['Label'] = df['Attack Type'].apply(lambda x: 'Benign' if x.lower() in ['normal', 'benign'] else 'Malicious')\n",
        "\n",
        "\n",
        "    # Additional corrections\n",
        "    df['Source Port'] = df['Source Port'].fillna(0).astype(int)\n",
        "    df['Destination Port'] = df['Destination Port'].fillna(0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "synthetic_data_df = postprocess_synthetic_data(synthetic_samples, processed_data, numerical_columns, categorical_columns)\n",
        "\n",
        "# Save to CSV\n",
        "synthetic_data_df.to_csv('synthetic_traffic_vae.csv', index=False)\n",
        "print(\"Postprocessed synthetic data saved to 'synthetic_traffic_vae.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOLWEvF1a0R8",
        "outputId": "17916598-ce4a-4ce9-d166-97e8624fb515"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 749.9640 - mean_squared_error: 0.2490 - val_loss: 742.9573 - val_mean_squared_error: 0.2467\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 740.7565 - mean_squared_error: 0.2459 - val_loss: 733.2805 - val_mean_squared_error: 0.2435\n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 730.5618 - mean_squared_error: 0.2425 - val_loss: 721.4537 - val_mean_squared_error: 0.2395\n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 716.8420 - mean_squared_error: 0.2380 - val_loss: 705.8759 - val_mean_squared_error: 0.2343\n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 700.8282 - mean_squared_error: 0.2326 - val_loss: 687.7731 - val_mean_squared_error: 0.2283\n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 681.1502 - mean_squared_error: 0.2261 - val_loss: 663.2711 - val_mean_squared_error: 0.2202\n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 659.2924 - mean_squared_error: 0.2188 - val_loss: 637.2678 - val_mean_squared_error: 0.2115\n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 628.2985 - mean_squared_error: 0.2085 - val_loss: 600.2484 - val_mean_squared_error: 0.1992\n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 589.1981 - mean_squared_error: 0.1954 - val_loss: 553.0396 - val_mean_squared_error: 0.1835\n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 542.7521 - mean_squared_error: 0.1799 - val_loss: 496.1613 - val_mean_squared_error: 0.1646\n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 482.2258 - mean_squared_error: 0.1597 - val_loss: 433.2852 - val_mean_squared_error: 0.1436\n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 412.6318 - mean_squared_error: 0.1364 - val_loss: 348.8224 - val_mean_squared_error: 0.1152\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 332.3834 - mean_squared_error: 0.1095 - val_loss: 262.4525 - val_mean_squared_error: 0.0863\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 250.0805 - mean_squared_error: 0.0817 - val_loss: 183.5233 - val_mean_squared_error: 0.0596\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 182.0886 - mean_squared_error: 0.0587 - val_loss: 129.3740 - val_mean_squared_error: 0.0409\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 107.6205 - mean_squared_error: 0.0335 - val_loss: 77.2983 - val_mean_squared_error: 0.0231\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 74.3837 - mean_squared_error: 0.0218 - val_loss: 54.7534 - val_mean_squared_error: 0.0148\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 53.5643 - mean_squared_error: 0.0143 - val_loss: 43.1781 - val_mean_squared_error: 0.0106\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 45.2995 - mean_squared_error: 0.0110 - val_loss: 37.9520 - val_mean_squared_error: 0.0083\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 38.7356 - mean_squared_error: 0.0085 - val_loss: 35.9756 - val_mean_squared_error: 0.0073\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 35.5078 - mean_squared_error: 0.0070 - val_loss: 31.7527 - val_mean_squared_error: 0.0056\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 30.7918 - mean_squared_error: 0.0053 - val_loss: 31.0202 - val_mean_squared_error: 0.0052\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 30.4713 - mean_squared_error: 0.0050 - val_loss: 29.8336 - val_mean_squared_error: 0.0047\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 28.6877 - mean_squared_error: 0.0044 - val_loss: 29.8406 - val_mean_squared_error: 0.0047\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 28.5197 - mean_squared_error: 0.0042 - val_loss: 28.3260 - val_mean_squared_error: 0.0042\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 29.5628 - mean_squared_error: 0.0046 - val_loss: 27.0082 - val_mean_squared_error: 0.0038\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 27.4325 - mean_squared_error: 0.0039 - val_loss: 27.7439 - val_mean_squared_error: 0.0040\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 26.7809 - mean_squared_error: 0.0037 - val_loss: 26.7749 - val_mean_squared_error: 0.0038\n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 26.5550 - mean_squared_error: 0.0038 - val_loss: 26.0910 - val_mean_squared_error: 0.0036\n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 26.7483 - mean_squared_error: 0.0039 - val_loss: 26.4189 - val_mean_squared_error: 0.0037\n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 25.6231 - mean_squared_error: 0.0035 - val_loss: 25.1241 - val_mean_squared_error: 0.0034\n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 25.2929 - mean_squared_error: 0.0034 - val_loss: 25.1609 - val_mean_squared_error: 0.0034\n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 24.4353 - mean_squared_error: 0.0032 - val_loss: 24.3522 - val_mean_squared_error: 0.0033\n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 24.4246 - mean_squared_error: 0.0033 - val_loss: 24.2394 - val_mean_squared_error: 0.0033\n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 24.3908 - mean_squared_error: 0.0034 - val_loss: 24.8770 - val_mean_squared_error: 0.0036\n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 23.7853 - mean_squared_error: 0.0033 - val_loss: 23.3550 - val_mean_squared_error: 0.0032\n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 24.4884 - mean_squared_error: 0.0036 - val_loss: 22.7928 - val_mean_squared_error: 0.0031\n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 23.1826 - mean_squared_error: 0.0033 - val_loss: 22.5508 - val_mean_squared_error: 0.0031\n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 22.9718 - mean_squared_error: 0.0032 - val_loss: 22.8495 - val_mean_squared_error: 0.0032\n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 22.1958 - mean_squared_error: 0.0031 - val_loss: 22.6968 - val_mean_squared_error: 0.0033\n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 22.3694 - mean_squared_error: 0.0032 - val_loss: 21.8088 - val_mean_squared_error: 0.0030\n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 22.2361 - mean_squared_error: 0.0032 - val_loss: 21.6909 - val_mean_squared_error: 0.0031\n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 21.1895 - mean_squared_error: 0.0029 - val_loss: 21.9255 - val_mean_squared_error: 0.0032\n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 21.3117 - mean_squared_error: 0.0030 - val_loss: 21.0963 - val_mean_squared_error: 0.0030\n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 21.1222 - mean_squared_error: 0.0031 - val_loss: 21.4512 - val_mean_squared_error: 0.0032\n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 20.9660 - mean_squared_error: 0.0031 - val_loss: 20.9935 - val_mean_squared_error: 0.0031\n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 20.6459 - mean_squared_error: 0.0030 - val_loss: 20.5663 - val_mean_squared_error: 0.0030\n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 21.0210 - mean_squared_error: 0.0032 - val_loss: 20.0249 - val_mean_squared_error: 0.0029\n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 20.0632 - mean_squared_error: 0.0029 - val_loss: 20.4932 - val_mean_squared_error: 0.0031\n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 20.1691 - mean_squared_error: 0.0030 - val_loss: 20.1252 - val_mean_squared_error: 0.0030\n",
            "Epoch 1/100, Generator Loss: 0.023347221314907074, Discriminator Loss: -0.038888897746801376\n",
            "Epoch 2/100, Generator Loss: -0.017779620364308357, Discriminator Loss: -0.09430389851331711\n",
            "Epoch 3/100, Generator Loss: 0.01733469031751156, Discriminator Loss: -0.2113647758960724\n",
            "Epoch 4/100, Generator Loss: 0.11994083970785141, Discriminator Loss: -0.3844490349292755\n",
            "Epoch 5/100, Generator Loss: 0.2470843344926834, Discriminator Loss: -0.6427700519561768\n",
            "Epoch 6/100, Generator Loss: 0.3512556552886963, Discriminator Loss: -0.9353355169296265\n",
            "Epoch 7/100, Generator Loss: 0.5130881667137146, Discriminator Loss: -1.2342479228973389\n",
            "Epoch 8/100, Generator Loss: 0.6577602028846741, Discriminator Loss: -1.6435279846191406\n",
            "Epoch 9/100, Generator Loss: 0.8154056668281555, Discriminator Loss: -2.0587172508239746\n",
            "Epoch 10/100, Generator Loss: 0.7702868580818176, Discriminator Loss: -2.3347983360290527\n",
            "Epoch 11/100, Generator Loss: 0.5284971594810486, Discriminator Loss: -2.482184410095215\n",
            "Epoch 12/100, Generator Loss: 0.37696123123168945, Discriminator Loss: -2.8079721927642822\n",
            "Epoch 13/100, Generator Loss: 0.5068637728691101, Discriminator Loss: -3.5901994705200195\n",
            "Epoch 14/100, Generator Loss: 0.8268547058105469, Discriminator Loss: -4.542661666870117\n",
            "Epoch 15/100, Generator Loss: 0.9737935066223145, Discriminator Loss: -5.23121976852417\n",
            "Epoch 16/100, Generator Loss: 0.6309103965759277, Discriminator Loss: -5.773324966430664\n",
            "Epoch 17/100, Generator Loss: -0.3455027639865875, Discriminator Loss: -5.395636558532715\n",
            "Epoch 18/100, Generator Loss: -2.817814826965332, Discriminator Loss: -3.296612024307251\n",
            "Epoch 19/100, Generator Loss: -6.302730560302734, Discriminator Loss: -0.6352324485778809\n",
            "Epoch 20/100, Generator Loss: -9.086201667785645, Discriminator Loss: 3.1075000762939453\n",
            "Epoch 21/100, Generator Loss: -8.794806480407715, Discriminator Loss: 5.034758567810059\n",
            "Epoch 22/100, Generator Loss: -2.006049156188965, Discriminator Loss: -2.192375659942627\n",
            "Epoch 23/100, Generator Loss: 6.778138637542725, Discriminator Loss: -10.829856872558594\n",
            "Epoch 24/100, Generator Loss: 12.873665809631348, Discriminator Loss: -17.431032180786133\n",
            "Epoch 25/100, Generator Loss: 15.053719520568848, Discriminator Loss: -18.252941131591797\n",
            "Epoch 26/100, Generator Loss: 9.37719440460205, Discriminator Loss: -12.556553840637207\n",
            "Epoch 27/100, Generator Loss: 7.896636486053467, Discriminator Loss: -9.474447250366211\n",
            "Epoch 28/100, Generator Loss: 14.910247802734375, Discriminator Loss: -17.705795288085938\n",
            "Epoch 29/100, Generator Loss: 19.595888137817383, Discriminator Loss: -19.701635360717773\n",
            "Epoch 30/100, Generator Loss: 17.97397804260254, Discriminator Loss: -17.899181365966797\n",
            "Epoch 31/100, Generator Loss: 8.495451927185059, Discriminator Loss: -10.53519344329834\n",
            "Epoch 32/100, Generator Loss: 2.381410598754883, Discriminator Loss: -4.831608295440674\n",
            "Epoch 33/100, Generator Loss: 1.1395052671432495, Discriminator Loss: -2.3277697563171387\n",
            "Epoch 34/100, Generator Loss: 5.482236385345459, Discriminator Loss: -6.859968662261963\n",
            "Epoch 35/100, Generator Loss: 12.354424476623535, Discriminator Loss: -13.10129451751709\n",
            "Epoch 36/100, Generator Loss: 14.314861297607422, Discriminator Loss: -14.231622695922852\n",
            "Epoch 37/100, Generator Loss: 12.588550567626953, Discriminator Loss: -12.415359497070312\n",
            "Epoch 38/100, Generator Loss: 7.029531478881836, Discriminator Loss: -7.970705986022949\n",
            "Epoch 39/100, Generator Loss: 3.152935028076172, Discriminator Loss: -3.9430713653564453\n",
            "Epoch 40/100, Generator Loss: 1.3929716348648071, Discriminator Loss: -4.098491668701172\n",
            "Epoch 41/100, Generator Loss: 1.2871661186218262, Discriminator Loss: -3.5916032791137695\n",
            "Epoch 42/100, Generator Loss: -0.32080385088920593, Discriminator Loss: -2.945841073989868\n",
            "Epoch 43/100, Generator Loss: -1.5137256383895874, Discriminator Loss: -2.6391372680664062\n",
            "Epoch 44/100, Generator Loss: -2.716181755065918, Discriminator Loss: -1.7305519580841064\n",
            "Epoch 45/100, Generator Loss: -3.8320248126983643, Discriminator Loss: -1.5914688110351562\n",
            "Epoch 46/100, Generator Loss: -4.463358402252197, Discriminator Loss: -0.9776792526245117\n",
            "Epoch 47/100, Generator Loss: -5.666568756103516, Discriminator Loss: -1.229248046875\n",
            "Epoch 48/100, Generator Loss: -5.745358943939209, Discriminator Loss: -1.1872992515563965\n",
            "Epoch 49/100, Generator Loss: -4.892973899841309, Discriminator Loss: -2.235034465789795\n",
            "Epoch 50/100, Generator Loss: -3.94563364982605, Discriminator Loss: -2.298867702484131\n",
            "Epoch 51/100, Generator Loss: -2.9226551055908203, Discriminator Loss: -3.9524736404418945\n",
            "Epoch 52/100, Generator Loss: -1.1074751615524292, Discriminator Loss: -5.213703155517578\n",
            "Epoch 53/100, Generator Loss: 1.7608776092529297, Discriminator Loss: -7.854382514953613\n",
            "Epoch 54/100, Generator Loss: 5.465102672576904, Discriminator Loss: -12.217106819152832\n",
            "Epoch 55/100, Generator Loss: 9.217381477355957, Discriminator Loss: -14.317300796508789\n",
            "Epoch 56/100, Generator Loss: 10.242801666259766, Discriminator Loss: -15.008224487304688\n",
            "Epoch 57/100, Generator Loss: 7.419600009918213, Discriminator Loss: -12.528582572937012\n",
            "Epoch 58/100, Generator Loss: 0.27974653244018555, Discriminator Loss: -2.8389408588409424\n",
            "Epoch 59/100, Generator Loss: -4.0081000328063965, Discriminator Loss: 2.6798458099365234\n",
            "Epoch 60/100, Generator Loss: -2.7117817401885986, Discriminator Loss: 0.9057526588439941\n",
            "Epoch 61/100, Generator Loss: 1.1674857139587402, Discriminator Loss: -1.6816902160644531\n",
            "Epoch 62/100, Generator Loss: 7.731381893157959, Discriminator Loss: -7.875766277313232\n",
            "Epoch 63/100, Generator Loss: 14.324627876281738, Discriminator Loss: -13.92611026763916\n",
            "Epoch 64/100, Generator Loss: 16.070451736450195, Discriminator Loss: -17.67000389099121\n",
            "Epoch 65/100, Generator Loss: 14.918551445007324, Discriminator Loss: -14.320363998413086\n",
            "Epoch 66/100, Generator Loss: 12.15649700164795, Discriminator Loss: -9.197587966918945\n",
            "Epoch 67/100, Generator Loss: 10.520523071289062, Discriminator Loss: -9.831414222717285\n",
            "Epoch 68/100, Generator Loss: 11.926483154296875, Discriminator Loss: -9.265510559082031\n",
            "Epoch 69/100, Generator Loss: 13.428301811218262, Discriminator Loss: -8.673785209655762\n",
            "Epoch 70/100, Generator Loss: 10.170578956604004, Discriminator Loss: -7.199480056762695\n",
            "Epoch 71/100, Generator Loss: 7.247727870941162, Discriminator Loss: -4.300144195556641\n",
            "Epoch 72/100, Generator Loss: 3.198103666305542, Discriminator Loss: -0.8202431201934814\n",
            "Epoch 73/100, Generator Loss: 0.761237621307373, Discriminator Loss: 0.36010849475860596\n",
            "Epoch 74/100, Generator Loss: -0.5940788984298706, Discriminator Loss: 0.808824896812439\n",
            "Epoch 75/100, Generator Loss: -1.0282533168792725, Discriminator Loss: 0.17184829711914062\n",
            "Epoch 76/100, Generator Loss: -1.6300163269042969, Discriminator Loss: -0.5665113925933838\n",
            "Epoch 77/100, Generator Loss: -1.536873459815979, Discriminator Loss: -1.5902589559555054\n",
            "Epoch 78/100, Generator Loss: -1.5304569005966187, Discriminator Loss: -2.128566265106201\n",
            "Epoch 79/100, Generator Loss: -1.7980159521102905, Discriminator Loss: -2.6482677459716797\n",
            "Epoch 80/100, Generator Loss: -1.675254464149475, Discriminator Loss: -3.303405284881592\n",
            "Epoch 81/100, Generator Loss: -1.5650869607925415, Discriminator Loss: -3.6230945587158203\n",
            "Epoch 82/100, Generator Loss: -1.6231327056884766, Discriminator Loss: -3.9120283126831055\n",
            "Epoch 83/100, Generator Loss: -1.490577220916748, Discriminator Loss: -4.2439656257629395\n",
            "Epoch 84/100, Generator Loss: -1.4161885976791382, Discriminator Loss: -4.662585258483887\n",
            "Epoch 85/100, Generator Loss: -1.866690754890442, Discriminator Loss: -4.5096116065979\n",
            "Epoch 86/100, Generator Loss: -2.5051145553588867, Discriminator Loss: -4.004886627197266\n",
            "Epoch 87/100, Generator Loss: -3.977208375930786, Discriminator Loss: -2.7763893604278564\n",
            "Epoch 88/100, Generator Loss: -5.59912633895874, Discriminator Loss: -0.6167984008789062\n",
            "Epoch 89/100, Generator Loss: -7.120047092437744, Discriminator Loss: 0.49309539794921875\n",
            "Epoch 90/100, Generator Loss: -7.120155334472656, Discriminator Loss: 0.4000263214111328\n",
            "Epoch 91/100, Generator Loss: -7.138418674468994, Discriminator Loss: 0.6529827117919922\n",
            "Epoch 92/100, Generator Loss: -6.396705627441406, Discriminator Loss: 0.739741325378418\n",
            "Epoch 93/100, Generator Loss: -6.766587734222412, Discriminator Loss: 1.3658428192138672\n",
            "Epoch 94/100, Generator Loss: -6.549646854400635, Discriminator Loss: 1.950944423675537\n",
            "Epoch 95/100, Generator Loss: -4.506354808807373, Discriminator Loss: 1.3635363578796387\n",
            "Epoch 96/100, Generator Loss: 2.3846333026885986, Discriminator Loss: -5.000748157501221\n",
            "Epoch 97/100, Generator Loss: 10.7236328125, Discriminator Loss: -12.551702499389648\n",
            "Epoch 98/100, Generator Loss: 16.59624481201172, Discriminator Loss: -19.635766983032227\n",
            "Epoch 99/100, Generator Loss: 10.84238338470459, Discriminator Loss: -12.1403169631958\n",
            "Epoch 100/100, Generator Loss: -0.1604253202676773, Discriminator Loss: -0.6130032539367676\n",
            "Postprocessed synthetic data saved to 'synthetic_traffic_vae.csv'.\n"
          ]
        }
      ]
    }
  ]
}