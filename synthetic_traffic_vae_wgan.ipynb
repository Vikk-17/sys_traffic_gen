{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxDFmBaIVMxPegcmUlz7r7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikk-17/sys_traffic_gen/blob/main/synthetic_traffic_vae_wgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# Preprocessing function with get_dummies\n",
        "scaler = None\n",
        "\n",
        "def preprocess_csv_with_dummies(file_path):\n",
        "    global scaler\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Detect numerical and categorical columns\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    if numerical_columns:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "    # One-hot encode categorical columns using pd.get_dummies\n",
        "    if categorical_columns:\n",
        "        df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "    # Convert the DataFrame to float32 for compatibility with TensorFlow\n",
        "    return df.astype(np.float32), numerical_columns, categorical_columns\n",
        "\n",
        "# Load and preprocess data\n",
        "file_path = '/content/synthetic_cicids_dataset_detailed.csv'\n",
        "\n",
        "processed_data, numerical_columns, categorical_columns = preprocess_csv_with_dummies(file_path)\n",
        "\n",
        "# Convert to NumPy array\n",
        "data = processed_data.values\n",
        "\n",
        "X_train, X_temp = train_test_split(data, test_size=0.4, random_state=42)\n",
        "X_valid, X_test = train_test_split(data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Define Variational Autoencoder components\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, original_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = self.build_encoder(original_dim, latent_dim)\n",
        "        self.decoder = self.build_decoder(original_dim, latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def build_encoder(self, original_dim, latent_dim):\n",
        "        inputs = tf.keras.layers.Input(shape=(original_dim,))\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(inputs)\n",
        "        x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "        z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return tf.keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    def build_decoder(self, original_dim, latent_dim):\n",
        "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "        x = tf.keras.layers.Dense(64, activation=\"relu\")(latent_inputs)\n",
        "        x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "        outputs = tf.keras.layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "        return tf.keras.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        reconstruction_loss = tf.keras.losses.mse(inputs, reconstructed)\n",
        "        reconstruction_loss *= tf.cast(tf.shape(inputs)[1], tf.float32)\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(tf.reduce_mean(reconstruction_loss + kl_loss))\n",
        "        return reconstructed\n",
        "\n",
        "\n",
        "# Define WGAN components\n",
        "class Generator(tf.keras.Model):\n",
        "    def __init__(self, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.dense3 = tf.keras.layers.Dense(data_dim, activation='tanh')\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense3 = tf.keras.layers.Dense(1)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "\n",
        "# WGAN training loop\n",
        "def train_wgan(generator, discriminator, data, latent_dim, batch_size=64, epochs=100, learning_rate=0.0001):\n",
        "    # Optimizers\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    # Training step\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, data.shape[0], batch_size):\n",
        "            real_data = data[i:i + batch_size]\n",
        "            batch_size_real = real_data.shape[0]\n",
        "\n",
        "            # Train Discriminator\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                z = tf.random.normal((batch_size_real, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "                real_output = discriminator(real_data, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "            grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "            disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
        "\n",
        "            # Train Generator\n",
        "            with tf.GradientTape() as gen_tape:\n",
        "                z = tf.random.normal((batch_size_real, latent_dim))\n",
        "                fake_data = generator(z, training=True)\n",
        "                fake_output = discriminator(fake_data, training=True)\n",
        "\n",
        "                gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "            grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "            gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}\")\n",
        "\n",
        "\n",
        "# Initialize generator, discriminator, and VAE\n",
        "original_dim = data.shape[1]\n",
        "latent_dim = 10\n",
        "vae_latent_dim = 5\n",
        "\n",
        "vae = VAE(original_dim, vae_latent_dim)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[tf.keras.losses.MeanSquaredError()])\n",
        "vae.fit(X_train, X_train, validation_data=(X_valid, X_valid), epochs=50, batch_size=64)\n",
        "\n",
        "generator = Generator(original_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Train WGAN\n",
        "train_wgan(generator, discriminator, X_train, latent_dim)\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_synthetic_data(generator, num_samples, latent_dim):\n",
        "    z = tf.random.normal((num_samples, latent_dim))\n",
        "    synthetic_data = generator(z, training=False).numpy()\n",
        "    return synthetic_data\n",
        "\n",
        "synthetic_samples = generate_synthetic_data(generator, num_samples=5000, latent_dim=latent_dim)\n",
        "\n",
        "\n",
        "# Transform synthetic data back to match original input\n",
        "def postprocess_synthetic_data(synthetic_data, original_df, numerical_columns, categorical_columns):\n",
        "    global scaler\n",
        "    df = pd.DataFrame(synthetic_data, columns=original_df.columns)\n",
        "\n",
        "    # columns_to_convert = [\"Packet Count\", \"Byte Count\", \"Flow Duration (ms)\", \"Idle Time (ms)\", \"Active Time (ms)\"]\n",
        "\n",
        "    # for column in columns_to_convert:\n",
        "    #     if column in numerical_columns:\n",
        "    #         df[column] = df[column].round().astype(int)\n",
        "\n",
        "    # Denormalize numerical columns\n",
        "    if numerical_columns and scaler:\n",
        "        df[numerical_columns] = scaler.inverse_transform(df[numerical_columns])\n",
        "\n",
        "        # Clip and round ports to valid range\n",
        "        if 'Source Port' in numerical_columns:\n",
        "            df['Source Port'] = df['Source Port'].clip(0, 65535).round().astype(int)\n",
        "        if 'Destination Port' in numerical_columns:\n",
        "            df['Destination Port'] = df['Destination Port'].clip(0, 65535).round().astype(int)\n",
        "\n",
        "\n",
        "        if \"Packet Count\" in numerical_columns:\n",
        "            df['Packet Count'] = df['Packet Count'].round().astype(int)\n",
        "        if \"Byte Count\" in numerical_columns:\n",
        "            df['Byte Count'] = df['Byte Count'].round().astype(int)\n",
        "        if \"Flow Duration (ms)\" in numerical_columns:\n",
        "            df['Flow Duration (ms)'] = df['Flow Duration (ms)'].round().astype(int)\n",
        "        if \"Idle Time (ms)\" in numerical_columns:\n",
        "            df['Idle Time (ms)'] = df['Idle Time (ms)'].round().astype(int)\n",
        "\n",
        "        if \"Active Time (ms)\" in numerical_columns:\n",
        "            df['Active Time (ms)'] = df['Active Time (ms)'].round().astype(int)\n",
        "\n",
        "\n",
        "    # Convert one-hot encoded columns back to original categories\n",
        "    for cat_col in categorical_columns:\n",
        "        cat_prefix = [col for col in df.columns if col.startswith(cat_col + '_')]\n",
        "        if cat_prefix:\n",
        "            df[cat_col] = df[cat_prefix].idxmax(axis=1).apply(lambda x: x.split('_', 1)[-1])\n",
        "            df = df.drop(columns=cat_prefix)\n",
        "\n",
        "\n",
        "    if 'Attack Type' in df.columns:\n",
        "        df['Label'] = df['Attack Type'].apply(lambda x: 'Benign' if x.lower() in ['normal', 'benign'] else 'Malicious')\n",
        "\n",
        "\n",
        "    df['Source Port'] = df['Source Port'].fillna(0).astype(int)\n",
        "    df['Destination Port'] = df['Destination Port'].fillna(0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "synthetic_data_df = postprocess_synthetic_data(synthetic_samples, processed_data, numerical_columns, categorical_columns)\n",
        "\n",
        "# Save to CSV\n",
        "synthetic_data_df.to_csv('synthetic_traffic_vae_wgan.csv', index=False)\n",
        "print(\"Postprocessed synthetic data saved to 'synthetic_traffic_vae_wgan.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOLWEvF1a0R8",
        "outputId": "87be95a6-529a-47a5-909b-da2126a8961e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - loss: 749.4598 - mean_squared_error: 0.2488 - val_loss: 742.3734 - val_mean_squared_error: 0.2465\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 739.8483 - mean_squared_error: 0.2456 - val_loss: 731.0573 - val_mean_squared_error: 0.2427\n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 728.8547 - mean_squared_error: 0.2419 - val_loss: 718.7036 - val_mean_squared_error: 0.2386\n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 715.2525 - mean_squared_error: 0.2374 - val_loss: 703.3266 - val_mean_squared_error: 0.2335\n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 699.1729 - mean_squared_error: 0.2321 - val_loss: 683.3559 - val_mean_squared_error: 0.2269\n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 679.0435 - mean_squared_error: 0.2253 - val_loss: 657.8299 - val_mean_squared_error: 0.2183\n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 650.2981 - mean_squared_error: 0.2157 - val_loss: 628.8753 - val_mean_squared_error: 0.2086\n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 619.3112 - mean_squared_error: 0.2054 - val_loss: 585.9862 - val_mean_squared_error: 0.1944\n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 576.3409 - mean_squared_error: 0.1911 - val_loss: 537.7836 - val_mean_squared_error: 0.1782\n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 520.1620 - mean_squared_error: 0.1723 - val_loss: 472.2832 - val_mean_squared_error: 0.1564\n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 456.5675 - mean_squared_error: 0.1510 - val_loss: 401.5655 - val_mean_squared_error: 0.1328\n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 386.0327 - mean_squared_error: 0.1271 - val_loss: 313.2305 - val_mean_squared_error: 0.1030\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 291.2161 - mean_squared_error: 0.0953 - val_loss: 234.4618 - val_mean_squared_error: 0.0762\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 219.8731 - mean_squared_error: 0.0709 - val_loss: 159.9844 - val_mean_squared_error: 0.0504\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 161.1691 - mean_squared_error: 0.0503 - val_loss: 119.8861 - val_mean_squared_error: 0.0359\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 114.4115 - mean_squared_error: 0.0337 - val_loss: 93.7359 - val_mean_squared_error: 0.0261\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 83.9016 - mean_squared_error: 0.0227 - val_loss: 71.4078 - val_mean_squared_error: 0.0176\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 67.7974 - mean_squared_error: 0.0163 - val_loss: 64.0404 - val_mean_squared_error: 0.0145\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 60.9928 - mean_squared_error: 0.0133 - val_loss: 58.6410 - val_mean_squared_error: 0.0122\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 54.0748 - mean_squared_error: 0.0105 - val_loss: 55.9765 - val_mean_squared_error: 0.0109\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 55.9331 - mean_squared_error: 0.0110 - val_loss: 48.5632 - val_mean_squared_error: 0.0084\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 50.3193 - mean_squared_error: 0.0091 - val_loss: 46.5538 - val_mean_squared_error: 0.0078\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 44.9639 - mean_squared_error: 0.0073 - val_loss: 46.4493 - val_mean_squared_error: 0.0080\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 43.9387 - mean_squared_error: 0.0072 - val_loss: 41.8078 - val_mean_squared_error: 0.0067\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 41.7543 - mean_squared_error: 0.0067 - val_loss: 39.8701 - val_mean_squared_error: 0.0062\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 39.0470 - mean_squared_error: 0.0061 - val_loss: 40.2647 - val_mean_squared_error: 0.0066\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 37.1331 - mean_squared_error: 0.0055 - val_loss: 39.1028 - val_mean_squared_error: 0.0064\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 36.1097 - mean_squared_error: 0.0056 - val_loss: 35.8383 - val_mean_squared_error: 0.0056\n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 36.5448 - mean_squared_error: 0.0059 - val_loss: 35.4759 - val_mean_squared_error: 0.0057\n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 34.1739 - mean_squared_error: 0.0053 - val_loss: 34.6767 - val_mean_squared_error: 0.0056\n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 33.1189 - mean_squared_error: 0.0051 - val_loss: 33.3742 - val_mean_squared_error: 0.0053\n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 34.6317 - mean_squared_error: 0.0058 - val_loss: 33.7405 - val_mean_squared_error: 0.0056\n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 30.6284 - mean_squared_error: 0.0046 - val_loss: 30.7613 - val_mean_squared_error: 0.0047\n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 32.3257 - mean_squared_error: 0.0052 - val_loss: 29.7673 - val_mean_squared_error: 0.0045\n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 30.7556 - mean_squared_error: 0.0049 - val_loss: 29.0990 - val_mean_squared_error: 0.0044\n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 27.1588 - mean_squared_error: 0.0038 - val_loss: 27.5839 - val_mean_squared_error: 0.0040\n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 27.0550 - mean_squared_error: 0.0039 - val_loss: 27.3151 - val_mean_squared_error: 0.0041\n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 26.5573 - mean_squared_error: 0.0039 - val_loss: 26.0034 - val_mean_squared_error: 0.0038\n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 26.5119 - mean_squared_error: 0.0040 - val_loss: 26.4416 - val_mean_squared_error: 0.0041\n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 27.3705 - mean_squared_error: 0.0045 - val_loss: 24.1537 - val_mean_squared_error: 0.0034\n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 26.2311 - mean_squared_error: 0.0041 - val_loss: 24.7401 - val_mean_squared_error: 0.0037\n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 23.9339 - mean_squared_error: 0.0035 - val_loss: 24.3388 - val_mean_squared_error: 0.0036\n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 24.5885 - mean_squared_error: 0.0038 - val_loss: 23.8196 - val_mean_squared_error: 0.0036\n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 23.2141 - mean_squared_error: 0.0034 - val_loss: 24.6352 - val_mean_squared_error: 0.0040\n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 22.9616 - mean_squared_error: 0.0034 - val_loss: 23.5848 - val_mean_squared_error: 0.0037\n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 22.7053 - mean_squared_error: 0.0035 - val_loss: 21.7372 - val_mean_squared_error: 0.0032\n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 22.1307 - mean_squared_error: 0.0034 - val_loss: 22.1815 - val_mean_squared_error: 0.0034\n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 21.8637 - mean_squared_error: 0.0033 - val_loss: 21.6810 - val_mean_squared_error: 0.0034\n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 20.6680 - mean_squared_error: 0.0031 - val_loss: 20.9667 - val_mean_squared_error: 0.0032\n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 22.0861 - mean_squared_error: 0.0036 - val_loss: 20.7636 - val_mean_squared_error: 0.0032\n",
            "Epoch 1/100, Generator Loss: -0.026250293478369713, Discriminator Loss: -0.010893546044826508\n",
            "Epoch 2/100, Generator Loss: -0.10460623353719711, Discriminator Loss: -0.006737753748893738\n",
            "Epoch 3/100, Generator Loss: -0.08801358938217163, Discriminator Loss: -0.11029462516307831\n",
            "Epoch 4/100, Generator Loss: -0.028881484642624855, Discriminator Loss: -0.29409125447273254\n",
            "Epoch 5/100, Generator Loss: 0.14512762427330017, Discriminator Loss: -0.595293402671814\n",
            "Epoch 6/100, Generator Loss: 0.3597446382045746, Discriminator Loss: -0.9874027371406555\n",
            "Epoch 7/100, Generator Loss: 0.5068626999855042, Discriminator Loss: -1.303064227104187\n",
            "Epoch 8/100, Generator Loss: 0.6041254997253418, Discriminator Loss: -1.6460161209106445\n",
            "Epoch 9/100, Generator Loss: 0.621496856212616, Discriminator Loss: -2.03000545501709\n",
            "Epoch 10/100, Generator Loss: 0.6613886952400208, Discriminator Loss: -2.521829605102539\n",
            "Epoch 11/100, Generator Loss: 0.8696513175964355, Discriminator Loss: -3.087200880050659\n",
            "Epoch 12/100, Generator Loss: 0.7563578486442566, Discriminator Loss: -3.523385763168335\n",
            "Epoch 13/100, Generator Loss: 0.6704230308532715, Discriminator Loss: -4.014258861541748\n",
            "Epoch 14/100, Generator Loss: 0.5646306276321411, Discriminator Loss: -4.694423675537109\n",
            "Epoch 15/100, Generator Loss: 0.5717276930809021, Discriminator Loss: -5.357876777648926\n",
            "Epoch 16/100, Generator Loss: 0.26586341857910156, Discriminator Loss: -6.024753093719482\n",
            "Epoch 17/100, Generator Loss: -1.56759512424469, Discriminator Loss: -4.863836765289307\n",
            "Epoch 18/100, Generator Loss: -4.71711540222168, Discriminator Loss: -2.210239887237549\n",
            "Epoch 19/100, Generator Loss: -9.196446418762207, Discriminator Loss: 1.9912185668945312\n",
            "Epoch 20/100, Generator Loss: -11.432934761047363, Discriminator Loss: 5.298906326293945\n",
            "Epoch 21/100, Generator Loss: -6.442094326019287, Discriminator Loss: 0.914276123046875\n",
            "Epoch 22/100, Generator Loss: 1.4359626770019531, Discriminator Loss: -7.440402984619141\n",
            "Epoch 23/100, Generator Loss: 9.749618530273438, Discriminator Loss: -15.125186920166016\n",
            "Epoch 24/100, Generator Loss: 13.799891471862793, Discriminator Loss: -19.657146453857422\n",
            "Epoch 25/100, Generator Loss: 13.847355842590332, Discriminator Loss: -18.023685455322266\n",
            "Epoch 26/100, Generator Loss: 3.688868284225464, Discriminator Loss: -9.408170700073242\n",
            "Epoch 27/100, Generator Loss: 6.686742305755615, Discriminator Loss: -10.149818420410156\n",
            "Epoch 28/100, Generator Loss: 11.878827095031738, Discriminator Loss: -14.44886589050293\n",
            "Epoch 29/100, Generator Loss: 18.110002517700195, Discriminator Loss: -19.246204376220703\n",
            "Epoch 30/100, Generator Loss: 19.12480354309082, Discriminator Loss: -20.826587677001953\n",
            "Epoch 31/100, Generator Loss: 16.026567459106445, Discriminator Loss: -18.307165145874023\n",
            "Epoch 32/100, Generator Loss: 5.791160583496094, Discriminator Loss: -10.14067268371582\n",
            "Epoch 33/100, Generator Loss: -2.5258491039276123, Discriminator Loss: -2.225219249725342\n",
            "Epoch 34/100, Generator Loss: 0.6652660369873047, Discriminator Loss: -3.6981911659240723\n",
            "Epoch 35/100, Generator Loss: 9.61837100982666, Discriminator Loss: -12.456476211547852\n",
            "Epoch 36/100, Generator Loss: 15.94153881072998, Discriminator Loss: -18.416126251220703\n",
            "Epoch 37/100, Generator Loss: 17.53867530822754, Discriminator Loss: -19.36858558654785\n",
            "Epoch 38/100, Generator Loss: 12.750408172607422, Discriminator Loss: -15.610321044921875\n",
            "Epoch 39/100, Generator Loss: 3.670567274093628, Discriminator Loss: -6.646744728088379\n",
            "Epoch 40/100, Generator Loss: 1.431025505065918, Discriminator Loss: -3.834609270095825\n",
            "Epoch 41/100, Generator Loss: 1.0607892274856567, Discriminator Loss: -4.474559307098389\n",
            "Epoch 42/100, Generator Loss: 0.4652388393878937, Discriminator Loss: -3.981783151626587\n",
            "Epoch 43/100, Generator Loss: 0.17378365993499756, Discriminator Loss: -3.5003843307495117\n",
            "Epoch 44/100, Generator Loss: -0.19206203520298004, Discriminator Loss: -2.8696460723876953\n",
            "Epoch 45/100, Generator Loss: -2.032310724258423, Discriminator Loss: -2.3764355182647705\n",
            "Epoch 46/100, Generator Loss: -2.4817044734954834, Discriminator Loss: -1.7547895908355713\n",
            "Epoch 47/100, Generator Loss: -2.3064024448394775, Discriminator Loss: -1.612574815750122\n",
            "Epoch 48/100, Generator Loss: -1.6232811212539673, Discriminator Loss: -3.119126558303833\n",
            "Epoch 49/100, Generator Loss: -0.260202556848526, Discriminator Loss: -4.923390865325928\n",
            "Epoch 50/100, Generator Loss: 0.22197073698043823, Discriminator Loss: -6.270657539367676\n",
            "Epoch 51/100, Generator Loss: 0.8390935063362122, Discriminator Loss: -5.771707534790039\n",
            "Epoch 52/100, Generator Loss: -0.1567998230457306, Discriminator Loss: -6.2241716384887695\n",
            "Epoch 53/100, Generator Loss: 1.992125391960144, Discriminator Loss: -8.15456485748291\n",
            "Epoch 54/100, Generator Loss: 3.705442428588867, Discriminator Loss: -11.38345718383789\n",
            "Epoch 55/100, Generator Loss: 7.865228176116943, Discriminator Loss: -13.655269622802734\n",
            "Epoch 56/100, Generator Loss: 10.199788093566895, Discriminator Loss: -16.988052368164062\n",
            "Epoch 57/100, Generator Loss: 8.9581298828125, Discriminator Loss: -15.191707611083984\n",
            "Epoch 58/100, Generator Loss: 0.7484641671180725, Discriminator Loss: -7.959899425506592\n",
            "Epoch 59/100, Generator Loss: -1.2980399131774902, Discriminator Loss: -5.817146301269531\n",
            "Epoch 60/100, Generator Loss: 1.072623372077942, Discriminator Loss: -6.771279811859131\n",
            "Epoch 61/100, Generator Loss: 1.2419397830963135, Discriminator Loss: -7.733105182647705\n",
            "Epoch 62/100, Generator Loss: -0.9987871050834656, Discriminator Loss: -4.249172210693359\n",
            "Epoch 63/100, Generator Loss: -4.208048343658447, Discriminator Loss: -0.20839309692382812\n",
            "Epoch 64/100, Generator Loss: -6.8131422996521, Discriminator Loss: 2.5468688011169434\n",
            "Epoch 65/100, Generator Loss: -4.968342304229736, Discriminator Loss: 0.6611847877502441\n",
            "Epoch 66/100, Generator Loss: 0.584708034992218, Discriminator Loss: -4.027780055999756\n",
            "Epoch 67/100, Generator Loss: 7.044297695159912, Discriminator Loss: -8.88277530670166\n",
            "Epoch 68/100, Generator Loss: 11.384971618652344, Discriminator Loss: -11.242019653320312\n",
            "Epoch 69/100, Generator Loss: 9.026423454284668, Discriminator Loss: -10.322765350341797\n",
            "Epoch 70/100, Generator Loss: 4.562098026275635, Discriminator Loss: -2.737624168395996\n",
            "Epoch 71/100, Generator Loss: 2.511169195175171, Discriminator Loss: -0.334067702293396\n",
            "Epoch 72/100, Generator Loss: 5.954074859619141, Discriminator Loss: -3.7081995010375977\n",
            "Epoch 73/100, Generator Loss: 11.616558074951172, Discriminator Loss: -9.206599235534668\n",
            "Epoch 74/100, Generator Loss: 14.806023597717285, Discriminator Loss: -12.014410972595215\n",
            "Epoch 75/100, Generator Loss: 12.48267650604248, Discriminator Loss: -10.292560577392578\n",
            "Epoch 76/100, Generator Loss: 7.847177028656006, Discriminator Loss: -6.496345043182373\n",
            "Epoch 77/100, Generator Loss: 2.336768388748169, Discriminator Loss: -1.619544506072998\n",
            "Epoch 78/100, Generator Loss: 1.601659893989563, Discriminator Loss: -0.7102235555648804\n",
            "Epoch 79/100, Generator Loss: 3.2607977390289307, Discriminator Loss: -3.287571668624878\n",
            "Epoch 80/100, Generator Loss: 5.842060089111328, Discriminator Loss: -6.300931930541992\n",
            "Epoch 81/100, Generator Loss: 6.240035533905029, Discriminator Loss: -7.7383294105529785\n",
            "Epoch 82/100, Generator Loss: 4.384393215179443, Discriminator Loss: -7.036731719970703\n",
            "Epoch 83/100, Generator Loss: 2.3731279373168945, Discriminator Loss: -5.406147480010986\n",
            "Epoch 84/100, Generator Loss: -1.2971311807632446, Discriminator Loss: -3.194355010986328\n",
            "Epoch 85/100, Generator Loss: -2.9878578186035156, Discriminator Loss: -2.506498336791992\n",
            "Epoch 86/100, Generator Loss: -3.4378280639648438, Discriminator Loss: -2.2123706340789795\n",
            "Epoch 87/100, Generator Loss: -4.449571132659912, Discriminator Loss: -2.106438636779785\n",
            "Epoch 88/100, Generator Loss: -5.417819499969482, Discriminator Loss: -2.2913336753845215\n",
            "Epoch 89/100, Generator Loss: -5.980783939361572, Discriminator Loss: -1.6429829597473145\n",
            "Epoch 90/100, Generator Loss: -6.61834716796875, Discriminator Loss: -1.5628466606140137\n",
            "Epoch 91/100, Generator Loss: -7.431922435760498, Discriminator Loss: -1.0744452476501465\n",
            "Epoch 92/100, Generator Loss: -7.3811116218566895, Discriminator Loss: -0.7081212997436523\n",
            "Epoch 93/100, Generator Loss: -7.807870388031006, Discriminator Loss: 0.40435791015625\n",
            "Epoch 94/100, Generator Loss: -7.3091912269592285, Discriminator Loss: 0.15390872955322266\n",
            "Epoch 95/100, Generator Loss: -6.72653341293335, Discriminator Loss: -0.5947146415710449\n",
            "Epoch 96/100, Generator Loss: -4.367649555206299, Discriminator Loss: -1.7088384628295898\n",
            "Epoch 97/100, Generator Loss: 0.3753126859664917, Discriminator Loss: -4.351116180419922\n",
            "Epoch 98/100, Generator Loss: 4.192991733551025, Discriminator Loss: -7.4325714111328125\n",
            "Epoch 99/100, Generator Loss: 5.4070143699646, Discriminator Loss: -7.876370906829834\n",
            "Epoch 100/100, Generator Loss: 5.945240497589111, Discriminator Loss: -4.894307613372803\n",
            "Postprocessed synthetic data saved to 'synthetic_traffic_vae_wgan.csv'.\n"
          ]
        }
      ]
    }
  ]
}