{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikk-17/sys_traffic_gen/blob/main/optimizedGan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a8ndp9nx77i0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q8PXJACd8sgC"
      },
      "outputs": [],
      "source": [
        "# Load multiple datasets\n",
        "def load_datasets(file_paths):\n",
        "  df_list = []\n",
        "  for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    df_list.append(df)\n",
        "  combined_df = pd.concat(df_list, ignore_index=True)\n",
        "  return combined_df\n",
        "\n",
        "\n",
        "def process_data(dataFrame):\n",
        "  \"\"\"\n",
        "  Takes data frame and returns features, labels.\n",
        "  :params: pandas data frame\n",
        "  :return: features: numpy array, labels: numpy array\n",
        "  \"\"\"\n",
        "  labels = dataFrame['Label'].copy()\n",
        "  features = dataFrame.drop(columns=['Label'])\n",
        "\n",
        "  # Encode labels to binary (0 for normal, 1 for attack)\n",
        "  # labels = (labels != 'BENIGN').astype(int)\n",
        "  # Encode labels to integers\n",
        "  label_encoder = LabelEncoder()\n",
        "  labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "  # Replce any infinity values with NaN\n",
        "  features = features.replace(to_replace=[-np.inf, np.inf], value=np.nan)\n",
        "\n",
        "  # Fill NaN values using suitable strategy (e.g., mean or median)\n",
        "  features = features.fillna(features.mean())\n",
        "\n",
        "  # Normalize features to a specific range [0, 1] both including\n",
        "  # x_scaled = (x-x_min) / (x_max-x_min)\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  features = scaler.fit_transform(features)\n",
        "\n",
        "  # split data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, scaler, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-coUnfvL5lV1"
      },
      "outputs": [],
      "source": [
        "# VAE Encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(units=128, activation='relu') # non linear activation\n",
        "    self.dense2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
        "\n",
        "    self.latent_mean = tf.keras.layers.Dense(latent_dim) # linear activation\n",
        "    self.latent_log_var = tf.keras.layers.Dense(latent_dim)\n",
        "\n",
        "\n",
        "  # returns the latent space (latent_mean, latent_log_var)\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    mean = self.latent_mean(x)\n",
        "    log_var = self.latent_log_var(x)\n",
        "    return mean, log_var\n",
        "\n",
        "\n",
        "# Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, feature_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(units=64, activation='relu')\n",
        "    self.dense2 = tf.keras.layers.Dense(units=128, activation='relu')\n",
        "    self.output_layer = tf.keras.layers.Dense(units=feature_dim, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    reconstructed = self.output_layer(x)\n",
        "    return reconstructed\n",
        "\n",
        "\n",
        "# Variational Autoencoder\n",
        "class VariationalAutoencoder(tf.keras.Model):\n",
        "  def __init__(self, latent_dim, feature_dim):\n",
        "    super(VariationalAutoencoder, self).__init__()\n",
        "    self.encoder = Encoder(latent_dim)\n",
        "    self.decoder = Decoder(feature_dim)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    mean, log_var = self.encoder(inputs)\n",
        "    epsilon = tf.random.normal(shape=tf.shape(mean))\n",
        "    z = mean + tf.exp(log_var * 0.5) * epsilon\n",
        "    reconstructed = self.decoder(z)\n",
        "    return reconstructed, mean, log_var\n",
        "\n",
        "\n",
        "def pretrain_vae(vae, data, epochs):\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(inputs):\n",
        "    with tf.GradientTape() as tape:\n",
        "      reconstructed, mean, log_var = vae(inputs)\n",
        "      reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.cast(inputs, tf.float32), reconstructed))\n",
        "      kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
        "      loss = reconstruction_loss + kl_loss\n",
        "    gradients = tape.gradient(loss, vae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
        "\n",
        "    # reconstruction accuracy\n",
        "    recon_accuracy = tf.reduce_mean(tf.cast(tf.abs(tf.cast(inputs, tf.float32) - reconstructed) < 0.1, tf.float32)) * 100\n",
        "    return loss, recon_accuracy\n",
        "\n",
        "  # for epoch in range(epochs):\n",
        "  #   for i in range(0, len(data), 64):\n",
        "  #     batch_data = data[i:i+64]\n",
        "  #     loss = train_step(batch_data)\n",
        "  #   print(f\"Epoch {epoch+1}, VAE Loss: {loss.numpy()}\")\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data).batch(64)\n",
        "\n",
        "\n",
        "    for batch in dataset:\n",
        "      loss, accuracy = train_step(batch)\n",
        "      epoch_loss += accuracy / len(dataset)\n",
        "      epoch_accuracy += accuracy / len(dataset)\n",
        "    print(f\"Epoch {epoch + 1}, VAE Loss: {loss.numpy()}, Reconstructed Accuracy: {epoch_accuracy.numpy()}%\")\n",
        "\n",
        "\n",
        "# def test_vae(vae, X_test, scaler):\n",
        "#   # scaler = MinMaxScaler()\n",
        "#   X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "#   reconstructed, _, _  = vae(X_test_normalized)\n",
        "\n",
        "#   # compute reconsturction loss\n",
        "#   reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(X_test_normalized, reconstructed))\n",
        "\n",
        "#   recon_accuracy = tf.reduce_mean(tf.cast(tf.abs(X_test_normalized - reconstructed)))\n",
        "\n",
        "#   print(f\"VAE Reconstruction Loss: {reconstruction_loss.numpy()}\")\n",
        "#   print(f\"VAE Reconstruction Accuracy: {recon_accuracy.numpy()}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IqKQGBsUqMX9"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "class WGANGenerator(tf.keras.Model):\n",
        "  def __init__(self, latent_dim, feature_dim):\n",
        "    super(WGANGenerator, self).__init__()\n",
        "    self.vae_decoder = Decoder(feature_dim)\n",
        "\n",
        "  def call(self, z):\n",
        "    return self.vae_decoder(z)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(units=256, activation='relu')\n",
        "    self.dense2 = tf.keras.layers.Dense(units=128, activation='relu')\n",
        "    self.output_layer = tf.keras.layers.Dense(units=1)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    return self.output_layer(x)\n",
        "\n",
        "def train_wgan(generator, discriminator, real_data, epochs, batch_size, latent_dim, clip_value=0.01):\n",
        "  gen_optimizer = tf.keras.optimizers.RMSprop(learning_rate=5e-5)\n",
        "  disc_optimizer = tf.keras.optimizers.RMSprop(learning_rate=5e-5)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(real_data):\n",
        "    noise = tf.random.normal([batch_size, latent_dim])\n",
        "\n",
        "    # Train discriminator\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      fake_data = generator(noise)\n",
        "      real_output = discriminator(real_data)\n",
        "      fake_output = discriminator(fake_data)\n",
        "      disc_loss = -(tf.reduce_mean(real_output) - tf.reduce_mean(fake_output))\n",
        "\n",
        "    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "    # Clip discriminator weights\n",
        "    for var in discriminator.trainable_variables:\n",
        "      var.assign(tf.clip_by_value(var, -clip_value, clip_value))\n",
        "\n",
        "    # Train generator\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      fake_data = generator(noise)\n",
        "      fake_output = discriminator(fake_data)\n",
        "      gen_loss = -tf.reduce_mean(fake_output)\n",
        "\n",
        "    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gen_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "\n",
        "    # Compute generator accuracy (percentage of fake data classified as real)\n",
        "    fake_predictions = tf.sigmoid(fake_output) > 0.5\n",
        "    gen_accuracy = tf.reduce_mean(tf.cast(fake_predictions, tf.float32)) * 100\n",
        "\n",
        "    return gen_loss, disc_loss, gen_accuracy\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_gen_loss = 0\n",
        "    epoch_disc_loss = 0\n",
        "    epoch_gen_accuracy = 0\n",
        "    num_batches = len(real_data) // batch_size\n",
        "\n",
        "    for i in range(0, len(real_data), batch_size):\n",
        "      real_batch = real_data[i:i+batch_size]\n",
        "      gen_loss, disc_loss, gen_accuracy = train_step(real_batch)\n",
        "      epoch_gen_loss += gen_loss / num_batches\n",
        "      epoch_disc_loss += disc_loss / num_batches\n",
        "      epoch_gen_accuracy += gen_accuracy / num_batches\n",
        "    print(f\"Epoch {epoch+1}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}, Generator Accuracy: {epoch_gen_accuracy.numpy()}%\")\n",
        "\n",
        "\n",
        "def test_wgan(generator, discriminator, X_test, latent_dim, scaler, batch_size=64):\n",
        "  # Normalize the test_data\n",
        "  real_data = scaler.transform(X_test)\n",
        "\n",
        "  # Generate the fake_data using the generator\n",
        "  noise = tf.random.normal([len(X_test), latent_dim])\n",
        "  fake_data = generator(noise)\n",
        "\n",
        "  # Evaluate the discriminator on real and fake data\n",
        "  real_output = discriminator(real_data)\n",
        "  fake_output = discriminator(fake_data)\n",
        "\n",
        "  # Calculate discriminator accuracy\n",
        "  real_accuracy = tf.reduce_mean(tf.cast(real_output > 0, tf.float32)) * 100\n",
        "  fake_accuracy = tf.reduce_mean(tf.cast(fake_output < 0, tf.float32)) * 100\n",
        "\n",
        "  print(f\"Discriminator Accuracy on Real Data: {real_accuracy.numpy()}%\")\n",
        "  print(f\"Discriminator Accuracy on Fake Data: {fake_accuracy.numpy()}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k9OxxkrnGOUs"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  file_paths = [\n",
        "      \"/content/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
        "      \"/content/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "      \"/content/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "      \"/content/Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "      \"/content/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "      \"/content/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
        "  ]\n",
        "\n",
        "  data = load_datasets(file_paths)\n",
        "\n",
        "  # split data\n",
        "  X_train, X_test, y_train, y_test, scaler, label_encoder = process_data(data)\n",
        "\n",
        "\n",
        "  latent_dim = 10\n",
        "  feature_dim = X_train.shape[1]\n",
        "\n",
        "  vae = VariationalAutoencoder(latent_dim, feature_dim)\n",
        "  pretrain_vae(vae, X_train, epochs=20)\n",
        "\n",
        "  # Test vae\n",
        "  # test_vae(vae, X_test, scaler)\n",
        "\n",
        "  generator = WGANGenerator(latent_dim, feature_dim)\n",
        "  discriminator = Discriminator()\n",
        "\n",
        "  train_wgan(generator, discriminator, X_train, epochs=50, batch_size=64, latent_dim=latent_dim)\n",
        "\n",
        "  # Test WGAN\n",
        "  # test_wgan(generator, discriminator, X_test, scaler, latent_dim)\n",
        "\n",
        "  # # Generate synthetic traffic\n",
        "  noise = tf.random.normal([1000, latent_dim])\n",
        "  synthetic_data = generator(noise).numpy()\n",
        "\n",
        "  synthetic_data = scaler.inverse_transform(synthetic_data)\n",
        "  pd.DataFrame(synthetic_data, columns=data.drop(columns=[\"Label\"]).columns).to_csv(\"Synthetic_traffic.csv\", index=False)\n",
        "\n",
        "  print(\"Synthetic traffic saved to 'synthetic_traffic.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HzBwmItMB2Se"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmJCyWzjZNPM5JNTXcJCR9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}